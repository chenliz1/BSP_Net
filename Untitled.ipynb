{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from bspnet2D import BspNet2D\n",
    "from loss_func import stage1_loss, rec_loss, stage2_loss\n",
    "from decoder2D import decoder2DStage2\n",
    "batchsize = 32\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['pixels']>\n",
      "Keys: <KeysViewHDF5 ['pixels']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('complex_elements.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    \n",
    "    \n",
    "with h5py.File('complex_elements_test.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    test_data = list(f[a_group_key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2000\n",
      "Testing Dataset Length: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Length: %d\" % len(data))\n",
    "print(\"Testing Dataset Length: %d\" % len(test_data))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batchsize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BspNet2D(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, _ = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage1_loss(output, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def test(net, test_ds):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        S,_ = net(example)\n",
    "        loss = stage1_loss(S, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "    return total_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(96.51742, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 50.306, Test_Loss: 96.518\n",
      "Epoch 001: Train_Loss: 50.298, Test_Loss: 96.511\n",
      "Epoch 002: Train_Loss: 50.290, Test_Loss: 96.511\n",
      "Epoch 003: Train_Loss: 50.284, Test_Loss: 96.504\n",
      "Epoch 004: Train_Loss: 50.278, Test_Loss: 96.503\n",
      "Epoch 005: Train_Loss: 50.272, Test_Loss: 96.503\n",
      "Epoch 006: Train_Loss: 50.266, Test_Loss: 96.501\n",
      "Epoch 007: Train_Loss: 50.260, Test_Loss: 96.500\n",
      "Epoch 008: Train_Loss: 50.255, Test_Loss: 96.501\n",
      "Epoch 009: Train_Loss: 50.251, Test_Loss: 96.493\n",
      "Epoch 010: Train_Loss: 50.245, Test_Loss: 96.497\n",
      "Epoch 011: Train_Loss: 50.240, Test_Loss: 96.488\n",
      "Epoch 012: Train_Loss: 50.234, Test_Loss: 96.486\n",
      "Epoch 013: Train_Loss: 50.227, Test_Loss: 96.487\n",
      "Epoch 014: Train_Loss: 50.223, Test_Loss: 96.488\n",
      "Epoch 015: Train_Loss: 50.218, Test_Loss: 96.487\n",
      "Epoch 016: Train_Loss: 50.212, Test_Loss: 96.484\n",
      "Epoch 017: Train_Loss: 50.207, Test_Loss: 96.483\n",
      "Epoch 018: Train_Loss: 50.202, Test_Loss: 96.480\n",
      "Epoch 019: Train_Loss: 50.197, Test_Loss: 96.479\n",
      "Epoch 020: Train_Loss: 50.192, Test_Loss: 96.474\n",
      "Epoch 021: Train_Loss: 50.189, Test_Loss: 96.469\n",
      "Epoch 022: Train_Loss: 50.183, Test_Loss: 96.470\n",
      "Epoch 023: Train_Loss: 50.177, Test_Loss: 96.468\n",
      "Epoch 024: Train_Loss: 50.173, Test_Loss: 96.467\n",
      "Epoch 025: Train_Loss: 50.166, Test_Loss: 96.465\n",
      "Epoch 026: Train_Loss: 50.162, Test_Loss: 96.465\n",
      "Epoch 027: Train_Loss: 50.156, Test_Loss: 96.467\n",
      "Epoch 028: Train_Loss: 50.150, Test_Loss: 96.461\n",
      "Epoch 029: Train_Loss: 50.144, Test_Loss: 96.462\n",
      "Epoch 030: Train_Loss: 50.138, Test_Loss: 96.457\n",
      "Epoch 031: Train_Loss: 50.135, Test_Loss: 96.457\n",
      "Epoch 032: Train_Loss: 50.130, Test_Loss: 96.452\n",
      "Epoch 033: Train_Loss: 50.125, Test_Loss: 96.459\n",
      "Epoch 034: Train_Loss: 50.119, Test_Loss: 96.453\n",
      "Epoch 035: Train_Loss: 50.114, Test_Loss: 96.454\n",
      "Epoch 036: Train_Loss: 50.110, Test_Loss: 96.450\n",
      "Epoch 037: Train_Loss: 50.103, Test_Loss: 96.448\n",
      "Epoch 038: Train_Loss: 50.099, Test_Loss: 96.449\n",
      "Epoch 039: Train_Loss: 50.094, Test_Loss: 96.442\n",
      "Epoch 040: Train_Loss: 50.089, Test_Loss: 96.443\n",
      "Epoch 041: Train_Loss: 50.084, Test_Loss: 96.440\n",
      "Epoch 042: Train_Loss: 50.077, Test_Loss: 96.441\n",
      "Epoch 043: Train_Loss: 50.072, Test_Loss: 96.439\n",
      "Epoch 044: Train_Loss: 50.068, Test_Loss: 96.438\n",
      "Epoch 045: Train_Loss: 50.063, Test_Loss: 96.437\n",
      "Epoch 046: Train_Loss: 50.058, Test_Loss: 96.434\n",
      "Epoch 047: Train_Loss: 50.054, Test_Loss: 96.441\n",
      "Epoch 048: Train_Loss: 50.050, Test_Loss: 96.434\n",
      "Epoch 049: Train_Loss: 50.044, Test_Loss: 96.433\n",
      "Epoch 050: Train_Loss: 50.040, Test_Loss: 96.430\n",
      "Epoch 051: Train_Loss: 50.034, Test_Loss: 96.425\n",
      "Epoch 052: Train_Loss: 50.030, Test_Loss: 96.425\n",
      "Epoch 053: Train_Loss: 50.023, Test_Loss: 96.422\n",
      "Epoch 054: Train_Loss: 50.018, Test_Loss: 96.413\n",
      "Epoch 055: Train_Loss: 50.013, Test_Loss: 96.418\n",
      "Epoch 056: Train_Loss: 50.007, Test_Loss: 96.415\n",
      "Epoch 057: Train_Loss: 50.002, Test_Loss: 96.411\n",
      "Epoch 058: Train_Loss: 49.998, Test_Loss: 96.405\n",
      "Epoch 059: Train_Loss: 49.990, Test_Loss: 96.408\n",
      "Epoch 060: Train_Loss: 49.987, Test_Loss: 96.409\n",
      "Epoch 061: Train_Loss: 49.981, Test_Loss: 96.407\n",
      "Epoch 062: Train_Loss: 49.975, Test_Loss: 96.408\n",
      "Epoch 063: Train_Loss: 49.973, Test_Loss: 96.406\n",
      "Epoch 064: Train_Loss: 49.967, Test_Loss: 96.403\n",
      "Epoch 065: Train_Loss: 49.961, Test_Loss: 96.407\n",
      "Epoch 066: Train_Loss: 49.956, Test_Loss: 96.406\n",
      "Epoch 067: Train_Loss: 49.951, Test_Loss: 96.401\n",
      "Epoch 068: Train_Loss: 49.945, Test_Loss: 96.396\n",
      "Epoch 069: Train_Loss: 49.940, Test_Loss: 96.399\n",
      "Epoch 070: Train_Loss: 49.934, Test_Loss: 96.391\n",
      "Epoch 071: Train_Loss: 49.929, Test_Loss: 96.385\n",
      "Epoch 072: Train_Loss: 49.924, Test_Loss: 96.386\n",
      "Epoch 073: Train_Loss: 49.918, Test_Loss: 96.381\n",
      "Epoch 074: Train_Loss: 49.914, Test_Loss: 96.383\n",
      "Epoch 075: Train_Loss: 49.908, Test_Loss: 96.377\n",
      "Epoch 076: Train_Loss: 49.901, Test_Loss: 96.374\n",
      "Epoch 077: Train_Loss: 49.897, Test_Loss: 96.376\n",
      "Epoch 078: Train_Loss: 49.892, Test_Loss: 96.376\n",
      "Epoch 079: Train_Loss: 49.887, Test_Loss: 96.372\n",
      "Epoch 080: Train_Loss: 49.881, Test_Loss: 96.370\n",
      "Epoch 081: Train_Loss: 49.876, Test_Loss: 96.368\n",
      "Epoch 082: Train_Loss: 49.872, Test_Loss: 96.373\n",
      "Epoch 083: Train_Loss: 49.866, Test_Loss: 96.364\n",
      "Epoch 084: Train_Loss: 49.863, Test_Loss: 96.365\n",
      "Epoch 085: Train_Loss: 49.858, Test_Loss: 96.363\n",
      "Epoch 086: Train_Loss: 49.853, Test_Loss: 96.361\n",
      "Epoch 087: Train_Loss: 49.847, Test_Loss: 96.356\n",
      "Epoch 088: Train_Loss: 49.841, Test_Loss: 96.357\n",
      "Epoch 089: Train_Loss: 49.836, Test_Loss: 96.353\n",
      "Epoch 090: Train_Loss: 49.831, Test_Loss: 96.352\n",
      "Epoch 091: Train_Loss: 49.826, Test_Loss: 96.354\n",
      "Epoch 092: Train_Loss: 49.820, Test_Loss: 96.352\n",
      "Epoch 093: Train_Loss: 49.816, Test_Loss: 96.350\n",
      "Epoch 094: Train_Loss: 49.810, Test_Loss: 96.346\n",
      "Epoch 095: Train_Loss: 49.804, Test_Loss: 96.346\n",
      "Epoch 096: Train_Loss: 49.799, Test_Loss: 96.346\n",
      "Epoch 097: Train_Loss: 49.794, Test_Loss: 96.335\n",
      "Epoch 098: Train_Loss: 49.788, Test_Loss: 96.336\n",
      "Epoch 099: Train_Loss: 49.783, Test_Loss: 96.336\n",
      "Epoch 100: Train_Loss: 49.777, Test_Loss: 96.339\n",
      "Epoch 101: Train_Loss: 49.771, Test_Loss: 96.327\n",
      "Epoch 102: Train_Loss: 49.767, Test_Loss: 96.328\n",
      "Epoch 103: Train_Loss: 49.762, Test_Loss: 96.330\n",
      "Epoch 104: Train_Loss: 49.757, Test_Loss: 96.324\n",
      "Epoch 105: Train_Loss: 49.752, Test_Loss: 96.322\n",
      "Epoch 106: Train_Loss: 49.747, Test_Loss: 96.322\n",
      "Epoch 107: Train_Loss: 49.741, Test_Loss: 96.321\n",
      "Epoch 108: Train_Loss: 49.737, Test_Loss: 96.312\n",
      "Epoch 109: Train_Loss: 49.733, Test_Loss: 96.319\n",
      "Epoch 110: Train_Loss: 49.728, Test_Loss: 96.314\n",
      "Epoch 111: Train_Loss: 49.723, Test_Loss: 96.312\n",
      "Epoch 112: Train_Loss: 49.718, Test_Loss: 96.305\n",
      "Epoch 113: Train_Loss: 49.713, Test_Loss: 96.306\n",
      "Epoch 114: Train_Loss: 49.707, Test_Loss: 96.303\n",
      "Epoch 115: Train_Loss: 49.703, Test_Loss: 96.299\n",
      "Epoch 116: Train_Loss: 49.699, Test_Loss: 96.299\n",
      "Epoch 117: Train_Loss: 49.694, Test_Loss: 96.301\n",
      "Epoch 118: Train_Loss: 49.688, Test_Loss: 96.296\n",
      "Epoch 119: Train_Loss: 49.682, Test_Loss: 96.295\n",
      "Epoch 120: Train_Loss: 49.678, Test_Loss: 96.290\n",
      "Epoch 121: Train_Loss: 49.674, Test_Loss: 96.294\n",
      "Epoch 122: Train_Loss: 49.668, Test_Loss: 96.291\n",
      "Epoch 123: Train_Loss: 49.664, Test_Loss: 96.287\n",
      "Epoch 124: Train_Loss: 49.659, Test_Loss: 96.289\n",
      "Epoch 125: Train_Loss: 49.654, Test_Loss: 96.286\n",
      "Epoch 126: Train_Loss: 49.648, Test_Loss: 96.287\n",
      "Epoch 127: Train_Loss: 49.644, Test_Loss: 96.282\n",
      "Epoch 128: Train_Loss: 49.638, Test_Loss: 96.280\n",
      "Epoch 129: Train_Loss: 49.633, Test_Loss: 96.280\n",
      "Epoch 130: Train_Loss: 49.627, Test_Loss: 96.280\n",
      "Epoch 131: Train_Loss: 49.623, Test_Loss: 96.280\n",
      "Epoch 132: Train_Loss: 49.618, Test_Loss: 96.278\n",
      "Epoch 133: Train_Loss: 49.614, Test_Loss: 96.277\n",
      "Epoch 134: Train_Loss: 49.609, Test_Loss: 96.274\n",
      "Epoch 135: Train_Loss: 49.605, Test_Loss: 96.271\n",
      "Epoch 136: Train_Loss: 49.600, Test_Loss: 96.273\n",
      "Epoch 137: Train_Loss: 49.596, Test_Loss: 96.274\n",
      "Epoch 138: Train_Loss: 49.592, Test_Loss: 96.271\n",
      "Epoch 139: Train_Loss: 49.587, Test_Loss: 96.264\n",
      "Epoch 140: Train_Loss: 49.581, Test_Loss: 96.261\n",
      "Epoch 141: Train_Loss: 49.578, Test_Loss: 96.260\n",
      "Epoch 142: Train_Loss: 49.574, Test_Loss: 96.261\n",
      "Epoch 143: Train_Loss: 49.569, Test_Loss: 96.259\n",
      "Epoch 144: Train_Loss: 49.565, Test_Loss: 96.255\n",
      "Epoch 145: Train_Loss: 49.563, Test_Loss: 96.257\n",
      "Epoch 146: Train_Loss: 49.555, Test_Loss: 96.255\n",
      "Epoch 147: Train_Loss: 49.552, Test_Loss: 96.257\n",
      "Epoch 148: Train_Loss: 49.548, Test_Loss: 96.254\n",
      "Epoch 149: Train_Loss: 49.546, Test_Loss: 96.256\n",
      "Epoch 150: Train_Loss: 49.541, Test_Loss: 96.259\n",
      "Epoch 151: Train_Loss: 49.536, Test_Loss: 96.256\n",
      "Epoch 152: Train_Loss: 49.534, Test_Loss: 96.256\n",
      "Epoch 153: Train_Loss: 49.529, Test_Loss: 96.255\n",
      "Epoch 154: Train_Loss: 49.525, Test_Loss: 96.255\n",
      "Epoch 155: Train_Loss: 49.521, Test_Loss: 96.252\n",
      "Epoch 156: Train_Loss: 49.518, Test_Loss: 96.246\n",
      "Epoch 157: Train_Loss: 49.514, Test_Loss: 96.249\n",
      "Epoch 158: Train_Loss: 49.509, Test_Loss: 96.249\n",
      "Epoch 159: Train_Loss: 49.506, Test_Loss: 96.248\n",
      "Epoch 160: Train_Loss: 49.503, Test_Loss: 96.251\n",
      "Epoch 161: Train_Loss: 49.498, Test_Loss: 96.251\n",
      "Epoch 162: Train_Loss: 49.494, Test_Loss: 96.250\n",
      "Epoch 163: Train_Loss: 49.489, Test_Loss: 96.247\n",
      "Epoch 164: Train_Loss: 49.486, Test_Loss: 96.251\n",
      "Epoch 165: Train_Loss: 49.483, Test_Loss: 96.250\n",
      "Epoch 166: Train_Loss: 49.479, Test_Loss: 96.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167: Train_Loss: 49.477, Test_Loss: 96.246\n",
      "Epoch 168: Train_Loss: 49.472, Test_Loss: 96.245\n",
      "Epoch 169: Train_Loss: 49.470, Test_Loss: 96.244\n",
      "Epoch 170: Train_Loss: 49.466, Test_Loss: 96.244\n",
      "Epoch 171: Train_Loss: 49.461, Test_Loss: 96.243\n",
      "Epoch 172: Train_Loss: 49.460, Test_Loss: 96.243\n",
      "Epoch 173: Train_Loss: 49.455, Test_Loss: 96.240\n",
      "Epoch 174: Train_Loss: 49.452, Test_Loss: 96.238\n",
      "Epoch 175: Train_Loss: 49.448, Test_Loss: 96.243\n",
      "Epoch 176: Train_Loss: 49.445, Test_Loss: 96.239\n",
      "Epoch 177: Train_Loss: 49.442, Test_Loss: 96.239\n",
      "Epoch 178: Train_Loss: 49.438, Test_Loss: 96.238\n",
      "Epoch 179: Train_Loss: 49.436, Test_Loss: 96.238\n",
      "Epoch 180: Train_Loss: 49.433, Test_Loss: 96.238\n",
      "Epoch 181: Train_Loss: 49.430, Test_Loss: 96.235\n",
      "Epoch 182: Train_Loss: 49.428, Test_Loss: 96.235\n",
      "Epoch 183: Train_Loss: 49.423, Test_Loss: 96.235\n",
      "Epoch 184: Train_Loss: 49.421, Test_Loss: 96.240\n",
      "Epoch 185: Train_Loss: 49.417, Test_Loss: 96.239\n",
      "Epoch 186: Train_Loss: 49.415, Test_Loss: 96.234\n",
      "Epoch 187: Train_Loss: 49.410, Test_Loss: 96.233\n",
      "Epoch 188: Train_Loss: 49.406, Test_Loss: 96.234\n",
      "Epoch 189: Train_Loss: 49.402, Test_Loss: 96.230\n",
      "Epoch 190: Train_Loss: 49.398, Test_Loss: 96.231\n",
      "Epoch 191: Train_Loss: 49.392, Test_Loss: 96.226\n",
      "Epoch 192: Train_Loss: 49.389, Test_Loss: 96.216\n",
      "Epoch 193: Train_Loss: 49.384, Test_Loss: 96.218\n",
      "Epoch 194: Train_Loss: 49.380, Test_Loss: 96.211\n",
      "Epoch 195: Train_Loss: 49.376, Test_Loss: 96.211\n",
      "Epoch 196: Train_Loss: 49.372, Test_Loss: 96.214\n",
      "Epoch 197: Train_Loss: 49.367, Test_Loss: 96.204\n",
      "Epoch 198: Train_Loss: 49.362, Test_Loss: 96.200\n",
      "Epoch 199: Train_Loss: 49.357, Test_Loss: 96.195\n",
      "Epoch 200: Train_Loss: 49.354, Test_Loss: 96.192\n",
      "Epoch 201: Train_Loss: 49.349, Test_Loss: 96.186\n",
      "Epoch 202: Train_Loss: 49.346, Test_Loss: 96.186\n",
      "Epoch 203: Train_Loss: 49.341, Test_Loss: 96.185\n",
      "Epoch 204: Train_Loss: 49.339, Test_Loss: 96.177\n",
      "Epoch 205: Train_Loss: 49.332, Test_Loss: 96.173\n",
      "Epoch 206: Train_Loss: 49.330, Test_Loss: 96.168\n",
      "Epoch 207: Train_Loss: 49.325, Test_Loss: 96.167\n",
      "Epoch 208: Train_Loss: 49.321, Test_Loss: 96.168\n",
      "Epoch 209: Train_Loss: 49.316, Test_Loss: 96.159\n",
      "Epoch 210: Train_Loss: 49.311, Test_Loss: 96.161\n",
      "Epoch 211: Train_Loss: 49.306, Test_Loss: 96.156\n",
      "Epoch 212: Train_Loss: 49.302, Test_Loss: 96.153\n",
      "Epoch 213: Train_Loss: 49.296, Test_Loss: 96.150\n",
      "Epoch 214: Train_Loss: 49.291, Test_Loss: 96.144\n",
      "Epoch 215: Train_Loss: 49.288, Test_Loss: 96.145\n",
      "Epoch 216: Train_Loss: 49.283, Test_Loss: 96.142\n",
      "Epoch 217: Train_Loss: 49.277, Test_Loss: 96.144\n",
      "Epoch 218: Train_Loss: 49.274, Test_Loss: 96.134\n",
      "Epoch 219: Train_Loss: 49.269, Test_Loss: 96.135\n",
      "Epoch 220: Train_Loss: 49.266, Test_Loss: 96.134\n",
      "Epoch 221: Train_Loss: 49.262, Test_Loss: 96.129\n",
      "Epoch 222: Train_Loss: 49.257, Test_Loss: 96.130\n",
      "Epoch 223: Train_Loss: 49.253, Test_Loss: 96.132\n",
      "Epoch 224: Train_Loss: 49.251, Test_Loss: 96.128\n",
      "Epoch 225: Train_Loss: 49.246, Test_Loss: 96.128\n",
      "Epoch 226: Train_Loss: 49.241, Test_Loss: 96.118\n",
      "Epoch 227: Train_Loss: 49.237, Test_Loss: 96.125\n",
      "Epoch 228: Train_Loss: 49.233, Test_Loss: 96.120\n",
      "Epoch 229: Train_Loss: 49.228, Test_Loss: 96.118\n",
      "Epoch 230: Train_Loss: 49.225, Test_Loss: 96.117\n",
      "Epoch 231: Train_Loss: 49.223, Test_Loss: 96.117\n",
      "Epoch 232: Train_Loss: 49.218, Test_Loss: 96.116\n",
      "Epoch 233: Train_Loss: 49.215, Test_Loss: 96.114\n",
      "Epoch 234: Train_Loss: 49.211, Test_Loss: 96.116\n",
      "Epoch 235: Train_Loss: 49.207, Test_Loss: 96.107\n",
      "Epoch 236: Train_Loss: 49.204, Test_Loss: 96.110\n",
      "Epoch 237: Train_Loss: 49.198, Test_Loss: 96.105\n",
      "Epoch 238: Train_Loss: 49.193, Test_Loss: 96.099\n",
      "Epoch 239: Train_Loss: 49.190, Test_Loss: 96.096\n",
      "Epoch 240: Train_Loss: 49.184, Test_Loss: 96.094\n",
      "Epoch 241: Train_Loss: 49.180, Test_Loss: 96.089\n",
      "Epoch 242: Train_Loss: 49.175, Test_Loss: 96.089\n",
      "Epoch 243: Train_Loss: 49.171, Test_Loss: 96.091\n",
      "Epoch 244: Train_Loss: 49.165, Test_Loss: 96.085\n",
      "Epoch 245: Train_Loss: 49.162, Test_Loss: 96.087\n",
      "Epoch 246: Train_Loss: 49.157, Test_Loss: 96.084\n",
      "Epoch 247: Train_Loss: 49.152, Test_Loss: 96.078\n",
      "Epoch 248: Train_Loss: 49.147, Test_Loss: 96.072\n",
      "Epoch 249: Train_Loss: 49.142, Test_Loss: 96.071\n",
      "Epoch 250: Train_Loss: 49.138, Test_Loss: 96.067\n",
      "Epoch 251: Train_Loss: 49.134, Test_Loss: 96.066\n",
      "Epoch 252: Train_Loss: 49.129, Test_Loss: 96.061\n",
      "Epoch 253: Train_Loss: 49.125, Test_Loss: 96.064\n",
      "Epoch 254: Train_Loss: 49.120, Test_Loss: 96.063\n",
      "Epoch 255: Train_Loss: 49.117, Test_Loss: 96.063\n",
      "Epoch 256: Train_Loss: 49.112, Test_Loss: 96.062\n",
      "Epoch 257: Train_Loss: 49.108, Test_Loss: 96.058\n",
      "Epoch 258: Train_Loss: 49.103, Test_Loss: 96.053\n",
      "Epoch 259: Train_Loss: 49.099, Test_Loss: 96.040\n",
      "Epoch 260: Train_Loss: 49.093, Test_Loss: 96.038\n",
      "Epoch 261: Train_Loss: 49.090, Test_Loss: 96.040\n",
      "Epoch 262: Train_Loss: 49.088, Test_Loss: 96.044\n",
      "Epoch 263: Train_Loss: 49.082, Test_Loss: 96.048\n",
      "Epoch 264: Train_Loss: 49.077, Test_Loss: 96.042\n",
      "Epoch 265: Train_Loss: 49.074, Test_Loss: 96.044\n",
      "Epoch 266: Train_Loss: 49.068, Test_Loss: 96.040\n",
      "Epoch 267: Train_Loss: 49.063, Test_Loss: 96.029\n",
      "Epoch 268: Train_Loss: 49.058, Test_Loss: 96.029\n",
      "Epoch 269: Train_Loss: 49.051, Test_Loss: 96.021\n",
      "Epoch 270: Train_Loss: 49.049, Test_Loss: 96.026\n",
      "Epoch 271: Train_Loss: 49.042, Test_Loss: 96.022\n",
      "Epoch 272: Train_Loss: 49.037, Test_Loss: 96.023\n",
      "Epoch 273: Train_Loss: 49.033, Test_Loss: 96.018\n",
      "Epoch 274: Train_Loss: 49.027, Test_Loss: 96.014\n",
      "Epoch 275: Train_Loss: 49.022, Test_Loss: 96.010\n",
      "Epoch 276: Train_Loss: 49.016, Test_Loss: 96.008\n",
      "Epoch 277: Train_Loss: 49.012, Test_Loss: 95.999\n",
      "Epoch 278: Train_Loss: 49.007, Test_Loss: 95.997\n",
      "Epoch 279: Train_Loss: 49.001, Test_Loss: 95.997\n",
      "Epoch 280: Train_Loss: 48.996, Test_Loss: 95.996\n",
      "Epoch 281: Train_Loss: 48.992, Test_Loss: 95.989\n",
      "Epoch 282: Train_Loss: 48.987, Test_Loss: 95.990\n",
      "Epoch 283: Train_Loss: 48.982, Test_Loss: 95.986\n",
      "Epoch 284: Train_Loss: 48.977, Test_Loss: 95.983\n",
      "Epoch 285: Train_Loss: 48.972, Test_Loss: 95.979\n",
      "Epoch 286: Train_Loss: 48.968, Test_Loss: 95.978\n",
      "Epoch 287: Train_Loss: 48.964, Test_Loss: 95.979\n",
      "Epoch 288: Train_Loss: 48.959, Test_Loss: 95.975\n",
      "Epoch 289: Train_Loss: 48.955, Test_Loss: 95.981\n",
      "Epoch 290: Train_Loss: 48.949, Test_Loss: 95.975\n",
      "Epoch 291: Train_Loss: 48.946, Test_Loss: 95.976\n",
      "Epoch 292: Train_Loss: 48.940, Test_Loss: 95.971\n",
      "Epoch 293: Train_Loss: 48.935, Test_Loss: 95.975\n",
      "Epoch 294: Train_Loss: 48.931, Test_Loss: 95.977\n",
      "Epoch 295: Train_Loss: 48.926, Test_Loss: 95.970\n",
      "Epoch 296: Train_Loss: 48.921, Test_Loss: 95.974\n",
      "Epoch 297: Train_Loss: 48.917, Test_Loss: 95.971\n",
      "Epoch 298: Train_Loss: 48.911, Test_Loss: 95.968\n",
      "Epoch 299: Train_Loss: 48.907, Test_Loss: 95.965\n",
      "Epoch 300: Train_Loss: 48.901, Test_Loss: 95.967\n",
      "Epoch 301: Train_Loss: 48.897, Test_Loss: 95.973\n",
      "Epoch 302: Train_Loss: 48.893, Test_Loss: 95.966\n",
      "Epoch 303: Train_Loss: 48.888, Test_Loss: 95.965\n",
      "Epoch 304: Train_Loss: 48.883, Test_Loss: 95.963\n",
      "Epoch 305: Train_Loss: 48.880, Test_Loss: 95.956\n",
      "Epoch 306: Train_Loss: 48.874, Test_Loss: 95.965\n",
      "Epoch 307: Train_Loss: 48.869, Test_Loss: 95.956\n",
      "Epoch 308: Train_Loss: 48.865, Test_Loss: 95.959\n",
      "Epoch 309: Train_Loss: 48.860, Test_Loss: 95.953\n",
      "Epoch 310: Train_Loss: 48.855, Test_Loss: 95.947\n",
      "Epoch 311: Train_Loss: 48.849, Test_Loss: 95.943\n",
      "Epoch 312: Train_Loss: 48.844, Test_Loss: 95.940\n",
      "Epoch 313: Train_Loss: 48.840, Test_Loss: 95.944\n",
      "Epoch 314: Train_Loss: 48.835, Test_Loss: 95.942\n",
      "Epoch 315: Train_Loss: 48.830, Test_Loss: 95.941\n",
      "Epoch 316: Train_Loss: 48.825, Test_Loss: 95.938\n",
      "Epoch 317: Train_Loss: 48.821, Test_Loss: 95.936\n",
      "Epoch 318: Train_Loss: 48.816, Test_Loss: 95.932\n",
      "Epoch 319: Train_Loss: 48.810, Test_Loss: 95.930\n",
      "Epoch 320: Train_Loss: 48.808, Test_Loss: 95.926\n",
      "Epoch 321: Train_Loss: 48.802, Test_Loss: 95.928\n",
      "Epoch 322: Train_Loss: 48.798, Test_Loss: 95.934\n",
      "Epoch 323: Train_Loss: 48.793, Test_Loss: 95.923\n",
      "Epoch 324: Train_Loss: 48.789, Test_Loss: 95.922\n",
      "Epoch 325: Train_Loss: 48.784, Test_Loss: 95.932\n",
      "Epoch 326: Train_Loss: 48.779, Test_Loss: 95.925\n",
      "Epoch 327: Train_Loss: 48.775, Test_Loss: 95.925\n",
      "Epoch 328: Train_Loss: 48.770, Test_Loss: 95.926\n",
      "Epoch 329: Train_Loss: 48.767, Test_Loss: 95.928\n",
      "Epoch 330: Train_Loss: 48.762, Test_Loss: 95.918\n",
      "Epoch 331: Train_Loss: 48.759, Test_Loss: 95.921\n",
      "Epoch 332: Train_Loss: 48.753, Test_Loss: 95.912\n",
      "Epoch 333: Train_Loss: 48.749, Test_Loss: 95.912\n",
      "Epoch 334: Train_Loss: 48.745, Test_Loss: 95.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335: Train_Loss: 48.740, Test_Loss: 95.917\n",
      "Epoch 336: Train_Loss: 48.737, Test_Loss: 95.920\n",
      "Epoch 337: Train_Loss: 48.734, Test_Loss: 95.914\n",
      "Epoch 338: Train_Loss: 48.728, Test_Loss: 95.912\n",
      "Epoch 339: Train_Loss: 48.724, Test_Loss: 95.910\n",
      "Epoch 340: Train_Loss: 48.719, Test_Loss: 95.907\n",
      "Epoch 341: Train_Loss: 48.715, Test_Loss: 95.905\n",
      "Epoch 342: Train_Loss: 48.711, Test_Loss: 95.903\n",
      "Epoch 343: Train_Loss: 48.706, Test_Loss: 95.906\n",
      "Epoch 344: Train_Loss: 48.701, Test_Loss: 95.903\n",
      "Epoch 345: Train_Loss: 48.698, Test_Loss: 95.910\n",
      "Epoch 346: Train_Loss: 48.693, Test_Loss: 95.896\n",
      "Epoch 347: Train_Loss: 48.689, Test_Loss: 95.892\n",
      "Epoch 348: Train_Loss: 48.686, Test_Loss: 95.904\n",
      "Epoch 349: Train_Loss: 48.681, Test_Loss: 95.902\n",
      "Epoch 350: Train_Loss: 48.676, Test_Loss: 95.906\n",
      "Epoch 351: Train_Loss: 48.672, Test_Loss: 95.901\n",
      "Epoch 352: Train_Loss: 48.668, Test_Loss: 95.896\n",
      "Epoch 353: Train_Loss: 48.665, Test_Loss: 95.901\n",
      "Epoch 354: Train_Loss: 48.661, Test_Loss: 95.897\n",
      "Epoch 355: Train_Loss: 48.657, Test_Loss: 95.889\n",
      "Epoch 356: Train_Loss: 48.652, Test_Loss: 95.892\n",
      "Epoch 357: Train_Loss: 48.649, Test_Loss: 95.890\n",
      "Epoch 358: Train_Loss: 48.644, Test_Loss: 95.888\n",
      "Epoch 359: Train_Loss: 48.640, Test_Loss: 95.893\n",
      "Epoch 360: Train_Loss: 48.636, Test_Loss: 95.884\n",
      "Epoch 361: Train_Loss: 48.632, Test_Loss: 95.885\n",
      "Epoch 362: Train_Loss: 48.627, Test_Loss: 95.882\n",
      "Epoch 363: Train_Loss: 48.623, Test_Loss: 95.882\n",
      "Epoch 364: Train_Loss: 48.620, Test_Loss: 95.884\n",
      "Epoch 365: Train_Loss: 48.619, Test_Loss: 95.887\n",
      "Epoch 366: Train_Loss: 48.613, Test_Loss: 95.890\n",
      "Epoch 367: Train_Loss: 48.610, Test_Loss: 95.889\n",
      "Epoch 368: Train_Loss: 48.607, Test_Loss: 95.887\n",
      "Epoch 369: Train_Loss: 48.602, Test_Loss: 95.883\n",
      "Epoch 370: Train_Loss: 48.598, Test_Loss: 95.882\n",
      "Epoch 371: Train_Loss: 48.595, Test_Loss: 95.877\n",
      "Epoch 372: Train_Loss: 48.591, Test_Loss: 95.877\n",
      "Epoch 373: Train_Loss: 48.587, Test_Loss: 95.876\n",
      "Epoch 374: Train_Loss: 48.585, Test_Loss: 95.876\n",
      "Epoch 375: Train_Loss: 48.581, Test_Loss: 95.878\n",
      "Epoch 376: Train_Loss: 48.577, Test_Loss: 95.878\n",
      "Epoch 377: Train_Loss: 48.574, Test_Loss: 95.874\n",
      "Epoch 378: Train_Loss: 48.570, Test_Loss: 95.873\n",
      "Epoch 379: Train_Loss: 48.567, Test_Loss: 95.874\n",
      "Epoch 380: Train_Loss: 48.563, Test_Loss: 95.868\n",
      "Epoch 381: Train_Loss: 48.558, Test_Loss: 95.872\n",
      "Epoch 382: Train_Loss: 48.557, Test_Loss: 95.872\n",
      "Epoch 383: Train_Loss: 48.551, Test_Loss: 95.873\n",
      "Epoch 384: Train_Loss: 48.547, Test_Loss: 95.876\n",
      "Epoch 385: Train_Loss: 48.543, Test_Loss: 95.878\n",
      "Epoch 386: Train_Loss: 48.539, Test_Loss: 95.874\n",
      "Epoch 387: Train_Loss: 48.535, Test_Loss: 95.872\n",
      "Epoch 388: Train_Loss: 48.530, Test_Loss: 95.869\n",
      "Epoch 389: Train_Loss: 48.527, Test_Loss: 95.869\n",
      "Epoch 390: Train_Loss: 48.523, Test_Loss: 95.867\n",
      "Epoch 391: Train_Loss: 48.519, Test_Loss: 95.869\n",
      "Epoch 392: Train_Loss: 48.514, Test_Loss: 95.866\n",
      "Epoch 393: Train_Loss: 48.510, Test_Loss: 95.865\n",
      "Epoch 394: Train_Loss: 48.505, Test_Loss: 95.859\n",
      "Epoch 395: Train_Loss: 48.502, Test_Loss: 95.859\n",
      "Epoch 396: Train_Loss: 48.498, Test_Loss: 95.859\n",
      "Epoch 397: Train_Loss: 48.493, Test_Loss: 95.843\n",
      "Epoch 398: Train_Loss: 48.488, Test_Loss: 95.847\n",
      "Epoch 399: Train_Loss: 48.485, Test_Loss: 95.842\n",
      "Epoch 400: Train_Loss: 48.479, Test_Loss: 95.837\n",
      "Epoch 401: Train_Loss: 48.476, Test_Loss: 95.836\n",
      "Epoch 402: Train_Loss: 48.472, Test_Loss: 95.836\n",
      "Epoch 403: Train_Loss: 48.468, Test_Loss: 95.831\n",
      "Epoch 404: Train_Loss: 48.466, Test_Loss: 95.831\n",
      "Epoch 405: Train_Loss: 48.462, Test_Loss: 95.829\n",
      "Epoch 406: Train_Loss: 48.456, Test_Loss: 95.830\n",
      "Epoch 407: Train_Loss: 48.452, Test_Loss: 95.824\n",
      "Epoch 408: Train_Loss: 48.448, Test_Loss: 95.824\n",
      "Epoch 409: Train_Loss: 48.443, Test_Loss: 95.820\n",
      "Epoch 410: Train_Loss: 48.439, Test_Loss: 95.827\n",
      "Epoch 411: Train_Loss: 48.436, Test_Loss: 95.828\n",
      "Epoch 412: Train_Loss: 48.432, Test_Loss: 95.820\n",
      "Epoch 413: Train_Loss: 48.428, Test_Loss: 95.820\n",
      "Epoch 414: Train_Loss: 48.423, Test_Loss: 95.819\n",
      "Epoch 415: Train_Loss: 48.419, Test_Loss: 95.818\n",
      "Epoch 416: Train_Loss: 48.415, Test_Loss: 95.818\n",
      "Epoch 417: Train_Loss: 48.412, Test_Loss: 95.815\n",
      "Epoch 418: Train_Loss: 48.407, Test_Loss: 95.813\n",
      "Epoch 419: Train_Loss: 48.402, Test_Loss: 95.810\n",
      "Epoch 420: Train_Loss: 48.398, Test_Loss: 95.809\n",
      "Epoch 421: Train_Loss: 48.394, Test_Loss: 95.804\n",
      "Epoch 422: Train_Loss: 48.390, Test_Loss: 95.802\n",
      "Epoch 423: Train_Loss: 48.388, Test_Loss: 95.807\n",
      "Epoch 424: Train_Loss: 48.383, Test_Loss: 95.807\n",
      "Epoch 425: Train_Loss: 48.378, Test_Loss: 95.801\n",
      "Epoch 426: Train_Loss: 48.376, Test_Loss: 95.803\n",
      "Epoch 427: Train_Loss: 48.373, Test_Loss: 95.798\n",
      "Epoch 428: Train_Loss: 48.369, Test_Loss: 95.797\n",
      "Epoch 429: Train_Loss: 48.365, Test_Loss: 95.792\n",
      "Epoch 430: Train_Loss: 48.361, Test_Loss: 95.794\n",
      "Epoch 431: Train_Loss: 48.360, Test_Loss: 95.796\n",
      "Epoch 432: Train_Loss: 48.355, Test_Loss: 95.794\n",
      "Epoch 433: Train_Loss: 48.353, Test_Loss: 95.794\n",
      "Epoch 434: Train_Loss: 48.349, Test_Loss: 95.793\n",
      "Epoch 435: Train_Loss: 48.346, Test_Loss: 95.799\n",
      "Epoch 436: Train_Loss: 48.342, Test_Loss: 95.794\n",
      "Epoch 437: Train_Loss: 48.338, Test_Loss: 95.795\n",
      "Epoch 438: Train_Loss: 48.334, Test_Loss: 95.793\n",
      "Epoch 439: Train_Loss: 48.330, Test_Loss: 95.792\n",
      "Epoch 440: Train_Loss: 48.327, Test_Loss: 95.781\n",
      "Epoch 441: Train_Loss: 48.322, Test_Loss: 95.782\n",
      "Epoch 442: Train_Loss: 48.318, Test_Loss: 95.782\n",
      "Epoch 443: Train_Loss: 48.313, Test_Loss: 95.774\n",
      "Epoch 444: Train_Loss: 48.310, Test_Loss: 95.770\n",
      "Epoch 445: Train_Loss: 48.307, Test_Loss: 95.773\n",
      "Epoch 446: Train_Loss: 48.304, Test_Loss: 95.769\n",
      "Epoch 447: Train_Loss: 48.301, Test_Loss: 95.767\n",
      "Epoch 448: Train_Loss: 48.296, Test_Loss: 95.770\n",
      "Epoch 449: Train_Loss: 48.294, Test_Loss: 95.768\n",
      "Epoch 450: Train_Loss: 48.289, Test_Loss: 95.772\n",
      "Epoch 451: Train_Loss: 48.285, Test_Loss: 95.769\n",
      "Epoch 452: Train_Loss: 48.284, Test_Loss: 95.767\n",
      "Epoch 453: Train_Loss: 48.278, Test_Loss: 95.774\n",
      "Epoch 454: Train_Loss: 48.277, Test_Loss: 95.772\n",
      "Epoch 455: Train_Loss: 48.274, Test_Loss: 95.772\n",
      "Epoch 456: Train_Loss: 48.270, Test_Loss: 95.768\n",
      "Epoch 457: Train_Loss: 48.266, Test_Loss: 95.761\n",
      "Epoch 458: Train_Loss: 48.262, Test_Loss: 95.761\n",
      "Epoch 459: Train_Loss: 48.259, Test_Loss: 95.761\n",
      "Epoch 460: Train_Loss: 48.257, Test_Loss: 95.751\n",
      "Epoch 461: Train_Loss: 48.255, Test_Loss: 95.760\n",
      "Epoch 462: Train_Loss: 48.251, Test_Loss: 95.755\n",
      "Epoch 463: Train_Loss: 48.248, Test_Loss: 95.756\n",
      "Epoch 464: Train_Loss: 48.246, Test_Loss: 95.756\n",
      "Epoch 465: Train_Loss: 48.243, Test_Loss: 95.756\n",
      "Epoch 466: Train_Loss: 48.240, Test_Loss: 95.756\n",
      "Epoch 467: Train_Loss: 48.238, Test_Loss: 95.750\n",
      "Epoch 468: Train_Loss: 48.235, Test_Loss: 95.753\n",
      "Epoch 469: Train_Loss: 48.233, Test_Loss: 95.747\n",
      "Epoch 470: Train_Loss: 48.231, Test_Loss: 95.747\n",
      "Epoch 471: Train_Loss: 48.230, Test_Loss: 95.743\n",
      "Epoch 472: Train_Loss: 48.228, Test_Loss: 95.742\n",
      "Epoch 473: Train_Loss: 48.225, Test_Loss: 95.738\n",
      "Epoch 474: Train_Loss: 48.224, Test_Loss: 95.740\n",
      "Epoch 475: Train_Loss: 48.224, Test_Loss: 95.733\n",
      "Epoch 476: Train_Loss: 48.221, Test_Loss: 95.733\n",
      "Epoch 477: Train_Loss: 48.221, Test_Loss: 95.736\n",
      "Epoch 478: Train_Loss: 48.221, Test_Loss: 95.732\n",
      "Epoch 479: Train_Loss: 48.221, Test_Loss: 95.738\n",
      "Epoch 480: Train_Loss: 48.222, Test_Loss: 95.731\n",
      "Epoch 481: Train_Loss: 48.225, Test_Loss: 95.737\n",
      "Epoch 482: Train_Loss: 48.225, Test_Loss: 95.737\n",
      "Epoch 483: Train_Loss: 48.225, Test_Loss: 95.739\n",
      "Epoch 484: Train_Loss: 48.227, Test_Loss: 95.745\n",
      "Epoch 485: Train_Loss: 48.226, Test_Loss: 95.748\n",
      "Epoch 486: Train_Loss: 48.226, Test_Loss: 95.749\n",
      "Epoch 487: Train_Loss: 48.228, Test_Loss: 95.747\n",
      "Epoch 488: Train_Loss: 48.229, Test_Loss: 95.753\n",
      "Epoch 489: Train_Loss: 48.228, Test_Loss: 95.761\n",
      "Epoch 490: Train_Loss: 48.230, Test_Loss: 95.767\n",
      "Epoch 491: Train_Loss: 48.228, Test_Loss: 95.774\n",
      "Epoch 492: Train_Loss: 48.225, Test_Loss: 95.783\n",
      "Epoch 493: Train_Loss: 48.219, Test_Loss: 95.784\n",
      "Epoch 494: Train_Loss: 48.213, Test_Loss: 95.779\n",
      "Epoch 495: Train_Loss: 48.207, Test_Loss: 95.781\n",
      "Epoch 496: Train_Loss: 48.198, Test_Loss: 95.783\n",
      "Epoch 497: Train_Loss: 48.190, Test_Loss: 95.775\n",
      "Epoch 498: Train_Loss: 48.183, Test_Loss: 95.775\n",
      "Epoch 499: Train_Loss: 48.177, Test_Loss: 95.770\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 500\n",
    "net.load_weights('stage1_version2_BS32')\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step(net, example, opt)\n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "        \n",
    "    test_loss = test(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss, test_loss))\n",
    "    train_loss_list.append(total_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('stage1_version2_BS32')\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(95.73111, shape=(), dtype=float32)\n",
      "(1, 4096, 64)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "net.decoder.switchStage(1)\n",
    "net.load_weights('stage1_version2_BS32')\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "em = tf.constant(test_data[11].reshape(1,64,64,1))\n",
    "ps, x2 = net(tf.cast(em,dtype=tf.float32))\n",
    "print(x2.shape)\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[11].reshape(64,64)\n",
    "ps[ps>1]=1\n",
    "ps[ps<=0.01]=0\n",
    "print(ps)\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb09c129850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAO2klEQVR4nO3dX4xc5X3G8e8T4z+BFBkn2HVtVBPJonARTLQCI1cRwSFxEQ25CFVoVVmVpb0hEVFSJaaVqqRqJbgJ9KJNZRUaX9AYEkJsWVGItcWqKlWGpRhi4zh2HBdWdlnSYEEj1bHJrxdz3E6W/TM7c/7N/J6PtJo5Z2b2/Lxnn33f95zj9ygiMLPR956mCzCzejjsZkk47GZJOOxmSTjsZkk47GZJDBR2SdskHZd0UtLOsooys/Kp3/PskpYAPwbuAKaA54F7I+KV8sozs7JcNsBnbwZORsQpAEl7gLuBOcO+TMtjBVcMsEkzm8//8At+Gec122uDhH0d8FrX8hRwy3wfWMEV3KKtA2zSzOZzKCbmfG2QsM/21+NdYwJJ48A4wAouH2BzZjaIQQ7QTQHXdC2vB87MfFNE7IqIsYgYW8ryATZnZoMYJOzPAxslXStpGfAZYF85ZZlZ2fruxkfERUmfBZ4BlgCPRcTR0iozs1INMmYnIr4HfK+kWsysQr6CziwJh90sCYfdLAmH3SwJh90sCYfdLAmH3SwJh90sCYfdLAmH3SyJgS6XtcE8c+Zw0yUA8Inf2tR0CVYDt+xmSTjsZkk47GZJOOxmSTjsZkk47GZJOOxmSTjsZkk47GZJOOxmSTjsZkk47GZJOOxmSTjsZkksGHZJj0malnSka90qSQcknSger6q2TDMbVC8t+zeAbTPW7QQmImIjMFEsm1mLLRj2iPgX4OczVt8N7C6e7wY+VXJdZlayfsfsayLiLEDxuLq8ksysCpVPSyVpHBgHWMHlVW/OzObQb8v+uqS1AMXj9FxvjIhdETEWEWNLWd7n5sxsUP2GfR+wvXi+HdhbTjlmVpVeTr19E/g34DpJU5J2AA8Cd0g6AdxRLJtZiy04Zo+Ie+d4aWvJtZhZhXwFnVkSDrtZEg67WRK+/dMMbbklU52a/Df71lP1cctuloTDbpaEw26WhMNuloTDbpaEw26WhMNuloTDbpaEw26WhMNuloTDbpaEw26WhMNuloTDbpaEw26WhMNuloQnr7CR1j0xR/aJMtyymyXhsJsl4bCbJeExu42U+SbPzD5+7+X2T9dIelbSMUlHJd1frF8l6YCkE8XjVdWXa2b96qUbfxH4YkRcD2wG7pN0A7ATmIiIjcBEsWxmLdXLvd7OAmeL529LOgasA+4Gbiveths4CHy5kirN5tHPvPczP5OhW7+oA3SSNgA3AYeANcUfgkt/EFaXXZyZlafnsEt6H/AU8PmIeGsRnxuXNClp8gLn+6nRzErQU9glLaUT9Mcj4jvF6tclrS1eXwtMz/bZiNgVEWMRMbaU5WXUbGZ9WHDMLknAo8CxiPha10v7gO3Ag8Xj3koqNJuhinvTZTgt18t59i3AHwM/lHTpJ/JndEL+pKQdwKvAPdWUaGZl6OVo/L8CmuPlreWWY2ZV8RV0NhTqvK30qJ6W87XxZkk47GZJuBs/Q51dtjq7pvNpaze1LT+fUTlS75bdLAmH3SwJh90sCY/ZrTXaMkafzzCflnPLbpaEw26WhLvx1qhh6LrPZ5hOy7llN0vCYTdLwmE3S8JjdqtV3WP07nF01dtu+/jdLbtZEg67WRLuxlvl6uy6z9d9bqpLP3PbTXHLbpaEw26WhLvxVom2dN17/UyGI/Vu2c2ScNjNknDYzZLwmN1K0fYx+mK+56ielluwZZe0QtJzkl6SdFTSV4v110o6JOmEpCckLau+XDPrVy/d+PPA7RFxI7AJ2CZpM/AQ8HBEbATeBHZUV6aZDaqXe70F8N/F4tLiK4DbgT8s1u8GvgJ8vfwSra2Gveve67ZG5bRcr/dnX1LcwXUaOAD8BDgXEReLt0wB66op0czK0FPYI+KdiNgErAduBq6f7W2zfVbSuKRJSZMXON9/pWY2kEWdeouIc8BBYDOwUtKlYcB64Mwcn9kVEWMRMbaU5YPUamYDWHDMLulq4EJEnJP0XuBjdA7OPQt8GtgDbAf2VlmoNa/JiSeaNCoTYPRynn0tsFvSEjo9gScjYr+kV4A9kv4KeBF4tNTKzKxUvRyNfxm4aZb1p+iM381sCKhzZq0eV2pV3KKttW3PBjeqp9fK0rafz6GY4K34uWZ7zdfGmyXhsJsl4f8IY+/Stq5pmw3TkXq37GZJOOxmSTjsZkl4zG4eo5ek7RNguGU3S8JhN0vC3XirdbKGNt4WqSxtHw65ZTdLwmE3S8JhN0vCY3Z7l2G6BLRpbR+nd3PLbpaEw26WhLvxNq+2XxVWt2Hqts/klt0sCYfdLAl3461no3pbpMXUUbXGb/9kZsPPYTdLwmE3SyLlmL0tY8FhN6pX2o3qba56btmL2za/KGl/sXytpEOSTkh6QtKy6so0s0Etpht/P3Csa/kh4OGI2Ai8CewoszAzK1dPt3+StB7YDfw18AXg94E3gN+MiIuSbgW+EhGfmO/7NHn7p167Zu7Wl2PYTlcNW71zKeP2T48AXwJ+VSy/HzgXEReL5Slg3UBVmlmlFgy7pLuA6Yh4oXv1LG+dtYsgaVzSpKTJC5zvs0wzG1QvR+O3AJ+UdCewAriSTku/UtJlReu+Hjgz24cjYhewCzrd+FKqNrNFW9QtmyXdBvxpRNwl6VvAUxGxR9LfAy9HxN/N9/k6x+xljME8fi9HG8fDo3p6rapbNn8Z+IKkk3TG8I8O8L3MrGKLuqgmIg4CB4vnp4Cbyy/JzKqwqG78oKruxlfdNXO3fnBNdp/bOJwoW1XdeDMbIg67WRJD3Y2vu0vYzV36cjS5D8vWht8Jd+PNzGE3y8JhN0ti6CavaMsYzxNglKOpU2NlGaZ975bdLAmH3SyJoejGt717Nwy3LRoGdc9L349h3rdu2c2ScNjNknDYzZJo5Zi9jWO1xfBpuXK05bTcqOxDt+xmSTjsZkm0phs/7F33ufi0XDnq7NKP6j5yy26WhMNulkRj3fhR7bYvxEfqB1fFlXYZ9oVbdrMkHHazJBx2syQaG7MPw/9wqkKGsWHd+jktl3E/9BR2SaeBt4F3gIsRMSZpFfAEsAE4DfxBRLxZTZlmNqjFdOM/GhGbImKsWN4JTETERmCiWDazlupp3viiZR+LiJ91rTsO3BYRZyWtBQ5GxHXzfZ9e540fpS59xu5im2Q71VnGvPEB/EDSC5LGi3VrIuIsQPG4evBSzawqvR6g2xIRZyStBg5I+lGvGyj+OIwDrODyPko0szL01LJHxJnicRp4ms6tml8vuu8Uj9NzfHZXRIxFxNhSlpdTtZkt2oItu6QrgPdExNvF848DfwnsA7YDDxaPe8sqathPy2UYGw4L74v/10s3fg3wtKRL7/+niPi+pOeBJyXtAF4F7qmuTDMb1IJhj4hTwI2zrP8voLxbsppZpVozecV82jIX2VzcVbRh4GvjzZJw2M2ScNjNkhiKMXu3tozfPU63YeOW3SwJh90siaHrxner+0o7d91tmLllN0vCYTdLYqi78TOVfaTe3XYbJW7ZzZJw2M2ScNjNkhipMXu3fk/LeZxuo8otu1kSDrtZEiPbjZ9prtNy7rZbFm7ZzZJw2M2ScNjNkkgzZu/mcbpl5JbdLAmH3SwJh90siZ7CLmmlpG9L+pGkY5JulbRK0gFJJ4rHq6ou1sz612vL/jfA9yPid+jcCuoYsBOYiIiNwESxbGYttWDYJV0JfAR4FCAifhkR54C7gd3F23YDn6qqSDMbXC8t+weBN4B/lPSipH8obt28JiLOAhSPqyus08wG1EvYLwM+DHw9Im4CfsEiuuySxiVNSpq8wPk+yzSzQfUS9ilgKiIOFcvfphP+1yWtBSgep2f7cETsioixiBhbyvIyajazPiwY9oj4T+A1SdcVq7YCrwD7gO3Fuu3A3koqNLNS9Hq57OeAxyUtA04Bf0LnD8WTknYArwL3VFOimZWhp7BHxGFgbJaXtpZbjplVxVfQmSXhsJsl4bCbJeGwmyXhsJsl4bCbJeGwmyWhiKhvY9IbwH8AHwB+VtuGZ9eGGsB1zOQ6ft1i6/jtiLh6thdqDfv/bVSajIjZLtJJVYPrcB111uFuvFkSDrtZEk2FfVdD2+3WhhrAdczkOn5daXU0MmY3s/q5G2+WRK1hl7RN0nFJJyXVNhutpMckTUs60rWu9qmwJV0j6dliOu6jku5vohZJKyQ9J+mloo6vFuuvlXSoqOOJYv6CyklaUsxvuL+pOiSdlvRDSYclTRbrmvgdqWza9trCLmkJ8LfA7wE3APdKuqGmzX8D2DZjXRNTYV8EvhgR1wObgfuKn0HdtZwHbo+IG4FNwDZJm4GHgIeLOt4EdlRcxyX305me/JKm6vhoRGzqOtXVxO9IddO2R0QtX8CtwDNdyw8AD9S4/Q3Aka7l48Da4vla4HhdtXTVsBe4o8lagMuBfwduoXPxxmWz7a8Kt7+++AW+HdgPqKE6TgMfmLGu1v0CXAn8lOJYWtl11NmNXwe81rU8VaxrSqNTYUvaANwEHGqilqLrfJjORKEHgJ8A5yLiYvGWuvbPI8CXgF8Vy+9vqI4AfiDpBUnjxbq690ul07bXGXbNsi7lqQBJ7wOeAj4fEW81UUNEvBMRm+i0rDcD18/2tiprkHQXMB0RL3SvrruOwpaI+DCdYeZ9kj5SwzZnGmja9oXUGfYp4Jqu5fXAmRq3P1NPU2GXTdJSOkF/PCK+02QtANG5u89BOscQVkq6NC9hHftnC/BJSaeBPXS68o80UAcRcaZ4nAaepvMHsO79MtC07QupM+zPAxuLI63LgM/QmY66KbVPhS1JdG6jdSwivtZULZKulrSyeP5e4GN0DgQ9C3y6rjoi4oGIWB8RG+j8PvxzRPxR3XVIukLSb1x6DnwcOELN+yWqnra96gMfMw403An8mM748M9r3O43gbPABTp/PXfQGRtOACeKx1U11PG7dLqkLwOHi687664F+BDwYlHHEeAvivUfBJ4DTgLfApbXuI9uA/Y3UUexvZeKr6OXfjcb+h3ZBEwW++a7wFVl1eEr6MyS8BV0Zkk47GZJOOxmSTjsZkk47GZJOOxmSTjsZkk47GZJ/C++wZoGaz8irgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb0600459d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.switchStage(2)\n",
    "net.decoder.stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(256, 64) dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 1., 1.],\n",
       "       [0., 1., 0., ..., 0., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.L2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_stage2(net, example, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, M = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage2_loss(output, F,M) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)\n",
    "\n",
    "def test_stage2(net, test_ds):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        output, M = net(example)\n",
    "        loss = stage2_loss(output, F,M) \n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "    return total_loss\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.0000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(165.32254, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 112.624, Test_Loss: 165.014\n",
      "model saved\n",
      "Epoch 001: Train_Loss: 111.849, Test_Loss: 165.398\n",
      "Epoch 002: Train_Loss: 111.615, Test_Loss: 165.605\n",
      "Epoch 003: Train_Loss: 111.487, Test_Loss: 165.663\n",
      "Epoch 004: Train_Loss: 111.391, Test_Loss: 165.735\n",
      "Epoch 005: Train_Loss: 111.323, Test_Loss: 165.807\n",
      "Epoch 006: Train_Loss: 111.267, Test_Loss: 165.851\n",
      "Epoch 007: Train_Loss: 111.223, Test_Loss: 165.862\n",
      "Epoch 008: Train_Loss: 111.183, Test_Loss: 165.849\n",
      "Epoch 009: Train_Loss: 111.147, Test_Loss: 165.858\n",
      "Epoch 010: Train_Loss: 111.116, Test_Loss: 165.852\n",
      "Epoch 011: Train_Loss: 111.085, Test_Loss: 165.897\n",
      "Epoch 012: Train_Loss: 111.060, Test_Loss: 165.902\n",
      "Epoch 013: Train_Loss: 111.034, Test_Loss: 165.920\n",
      "Epoch 014: Train_Loss: 111.009, Test_Loss: 165.938\n",
      "Epoch 015: Train_Loss: 110.986, Test_Loss: 165.953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d7a493ff0279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_stage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1955\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;34m\"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     reduced_grads = distribution.extended.batch_reduce_to(\n\u001b[0;32m--> 449\u001b[0;31m         ds_reduce_util.ReduceOp.SUM, grads_and_vars)\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mbatch_reduce_to\u001b[0;34m(self, reduce_op, value_destination_pairs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m       \u001b[0mreduce_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_batch_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_batch_reduce_to\u001b[0;34m(self, reduce_op, value_destination_pairs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return [\n\u001b[1;32m   1498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m     ]\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return [\n\u001b[1;32m   1498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m     ]\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mreduce_to\u001b[0;34m(self, reduce_op, value, destinations)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \"\"\"\n\u001b[1;32m   1465\u001b[0m     \u001b[0;31m# TODO(josh11b): More docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m     \u001b[0m_require_cross_replica_or_default_context_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableAggregation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_require_cross_replica_or_default_context_extended\u001b[0;34m(extended)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_require_cross_replica_or_default_context_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;34m\"\"\"Verify in cross-replica context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m   \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m   \u001b[0mcross_replica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_replica_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcross_replica\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcross_replica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribution_strategy_context.py\u001b[0m in \u001b[0;36m_get_per_thread_mode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_default_replica_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_distribution_strategy_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4929\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distribution_strategy_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4930\u001b[0m     \u001b[0;34m\"\"\"A stack to maintain distribution strategy context for each thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4931\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_distribution_strategy_stack\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4932\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4933\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy_stack\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "epoch = 100\n",
    "net.load_weights('stage2_version0_BS32')\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "prev_test_loss = test_stage2(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step_stage2(net, example, opt)\n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "        \n",
    "    test_loss = test_stage2(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss, test_loss))\n",
    "    train_loss_list.append(total_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('stage2_version0_BS32')\n",
    "        print(\"model saved\")\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(165.32254, shape=(), dtype=float32)\n",
      "1.0674446\n",
      "tf.Tensor(\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]], shape=(1, 4096, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# net.decoder.switchStage(2)\n",
    "net.load_weights('stage2_version0_BS32')\n",
    "prev_test_loss = test_stage2(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "em = tf.constant(test_data[11].reshape(1,64,64,1))\n",
    "\n",
    "ps, x2 = net(tf.cast(em,dtype=tf.float32))\n",
    "print(net.decoder.L3.weight.numpy().max())\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[11].reshape(64,64)\n",
    "print(x2)\n",
    "ps[ps>0]=1\n",
    "ps[ps<=0]=0\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb020722550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPB0lEQVR4nO3dX4xU53nH8e8va6jTpBYGL4gCLY6EUvsixtGKEFFFDsQJdaOQC7uKE1UoQuLGrRwlVQytVCVVK9k3sXtRWVrVbvbCDXb+uCAUxUEbo6pShb2ucQImDoRSs4J4CTGK24vUbJ5ezLvVZLx/zs6cc+bMvL+PtDpzzsxwHvbMM+/znvPuexQRmNnwe1e/AzCzejjZzTLhZDfLhJPdLBNOdrNMONnNMtFTskvaLek1SeckHSgrKDMrn7q9zi5pBPgJcDcwDbwI3B8Rr5YXnpmV5YYe3rsNOBcR5wEkHQL2AAsm+y2rR2LzphU97NLMFnPh4tv8/Bezmu+5XpJ9A3CxbX0a+NBib9i8aQUvPLeph12a2WK2feLigs/10mef79vjHX0CSfslTUmaunJ1tofdmVkvekn2aaC9md4IXOp8UUSMR8RYRIyNrhnpYXdm1otekv1FYIukWyWtBD4DHCknLDMrW9d99oi4LunPgOeAEeDJiDhdWmRmVqpeTtAREd8FvltSLGZWIY+gM8uEk90sE052s0w42c0y4WQ3y4ST3SwTTnazTPR0nd364xO/u3XZ73nu0skKIrFB4pbdLBNOdrNMuIxvkG7Kc7Oi3LKbZcLJbpYJl/F95LLd6uSW3SwTTnazTDjZzTLhZDfLhJPdLBNOdrNMONnNMuFkN8uEk90sE052s0x4uGwmOofmejKL/CzZskt6UtKMpFNt21ZLOibpbFreXG2YZtarImX814HdHdsOAJMRsQWYTOtm1mBLJntE/Cvwi47Ne4CJ9HgC+HTJcZlZybo9QbcuIi4DpOXa8kIysypUfjZe0n5JU5KmrlydrXp3ZraAbpP9DUnrAdJyZqEXRsR4RIxFxNjompEud2dmveo22Y8Ae9PjvcDhcsIxs6oUufT2DeDfgfdLmpa0D3gYuFvSWeDutG5mDbbkoJqIuH+Bp3aVHIuZVcjDZc0y4WQ3y4ST3SwT/kOYDrnM5b7Y/9N/JDOc3LKbZcLJbpYJJ7tZJtxnt3co47yF+/3N45bdLBNOdrNMuIy3SrR3BVzSN4NbdrNMONnNMuEy3irnkr4Z3LKbZcLJbpYJJ7tZJpzsZplwsptlwslulgknu1kmnOxmmXCym2XCyW6WCQ+XtYHnyTOLKXL7p02Snpd0RtJpSQ+m7aslHZN0Ni1vrj5cM+tWkTL+OvCliLgN2A48IOl24AAwGRFbgMm0bmYNVeReb5eBy+nxW5LOABuAPcBd6WUTwHHgoUqiNFtEe6neWdIvVOLnWN4v6wSdpM3AncAJYF36Ipj7QlhbdnBmVp7CyS7pvcC3gS9ExC+X8b79kqYkTV25OttNjGZWgkLJLmkFrUR/KiK+kza/IWl9en49MDPfeyNiPCLGImJsdM1IGTGbWReW7LNLEvAEcCYivtb21BFgL/BwWh6uJEIbeFX3j7uZ5z7Hy3VFrrPvAP4U+JGkud/CX9JK8mck7QNeB+6rJkQzK0ORs/H/BmiBp3eVG46ZVcUj6DosdhlnWA1C2Vrnsejc1yD8forw2HizTDjZzTLhMn4RTTyLXIWmlK1N+X10GpZ5792ym2XCyW6WCSe7WSbcZ7d3qLOP2tR++kKacn6jG27ZzTLhZDfLhMt4W9SgX3ZaKOayug+D9Ptxy26WCSe7WSac7GaZcJ/dalX1pbai/ebO15URV9P7727ZzTLhZDfLhMt4W1QZ5WhTSvc6NXGknVt2s0w42c0y4TLe3qHppXsVJXHVcw824Uy9W3azTDjZzTLhZDfLhPvs1vg+OtTbz61idN1i/15d/7clW3ZJN0p6QdIrkk5L+mrafqukE5LOSnpa0srqwzWzbhUp438F7IyIO4CtwG5J24FHgEcjYgvwJrCvujDNrFdF7vUWwH+n1RXpJ4CdwGfT9gngK8Dj5Yc4vKouF5ez7yZqSozDclmu6P3ZR9IdXGeAY8BPgWsRcT29ZBrYUE2IZlaGQskeEbMRsRXYCGwDbpvvZfO9V9J+SVOSpq5cne0+UjPrybIuvUXENeA4sB1YJWmuG7ARuLTAe8YjYiwixkbXjPQSq5n1YMk+u6RR4O2IuCbp3cDHaJ2cex64FzgE7AUOVxloDgbtdtGDEGPZ6rwsV3b/vch19vXAhKQRWpXAMxFxVNKrwCFJfwu8DDxRamRmVqoiZ+N/CNw5z/bztPrvZjYAPIIuE025jLWYQYixU5Vdr7JH2nlsvFkmnOxmmXAZ31BllIdll8VVnH0fxNK9X3o9U++W3SwTTnazTDjZzTLhPvsAWM6orab3gZseXy+aPgLSLbtZJpzsZplwGT+AhrkUHhZNnMfOLbtZJpzsZplwsptlwn12q1UTb2VchSYOLXbLbpYJJ7tZJlzGW2FVXE5qwq2Me1F2ud73eePNbPA52c0y4TLebBmaeJa9KLfsZplwsptlwslulgn32a1rZU/W0NTRdYN0eW0xhVv2dNvmlyUdTeu3Sjoh6aykpyWtrC5MM+vVcsr4B4EzbeuPAI9GxBbgTWBfmYGZWbkKlfGSNgJ/DPwd8EVJAnYCn00vmQC+AjxeQYylG+Q53JpqmEbXDfLltcUUbdkfA74M/DqtrwGuRcT1tD4NbCg5NjMr0ZLJLumTwExEvNS+eZ6XxgLv3y9pStLUlauzXYZpZr0qUsbvAD4l6R7gRuAmWi39Kkk3pNZ9I3BpvjdHxDgwDjB2x43zfiGYWfWK3J/9IHAQQNJdwF9ExOckfRO4FzgE7AUOVxhnz4r2wwb9r7CaosrLclUclybO8162XgbVPETrZN05Wn34J8oJycyqsKxBNRFxHDieHp8HtpUfkplVYWhH0OVQlg2KqkfaFd13t/9GN5rYBfTYeLNMONnNMjFUZXwTbrFji1vsd1hFaV1nd67pnw+37GaZcLKbZcLJbpaJge6z1315zaPrqlX1bY6rMEifA7fsZplwsptlYuDK+KaUdi7pq1f2yLsyDPKxdstulgknu1kmnOxmmRiIPntT+msL8bDa6tXZfx/W4+eW3SwTTnazTDSyjG962W79NYgj7ZrALbtZJpzsZploTBk/TKWYR9fVq4wz9TkcJ7fsZplwsptlwslulonG9NnNylD0slwOffRORe/PfgF4C5gFrkfEmKTVwNPAZuAC8CcR8WY1YZpZr5ZTxn80IrZGxFhaPwBMRsQWYDKtm1lD9VLG7wHuSo8naN0D7qFu/7EmTlRggy/Hcn0hRVv2AL4v6SVJ+9O2dRFxGSAt11YRoJmVo2jLviMiLklaCxyT9OOiO0hfDvsBfm+Dzwea9Uuhlj0iLqXlDPAsrVs1vyFpPUBazizw3vGIGIuIsdE1I+VEbWbLtmRTK+k9wLsi4q30+OPA3wBHgL3Aw2l5uKygBv2vmtxPtCYqUlevA56VNPf6f46I70l6EXhG0j7gdeC+6sI0s14tmewRcR64Y57tV4FdVQRlZuUbiDNmTb8s57LdBoHHxptlwslulgknu1kmBqLP3q4p/Xf3023QuGU3y4ST3SwTA1fGt6t7pJ1LdxtkbtnNMuFkN8vEQJfxnco+U++y3YaJW3azTDjZzTLhZDfLxFD12cvgfroNK7fsZplwsptlYmjL+MXKcd9S2XLklt0sE052s0w42c0yMbR99sW4n245cstulgknu1kmnOxmmSiU7JJWSfqWpB9LOiPpw5JWSzom6Wxa3lx1sGbWvaIt+98D34uIP6B1K6gzwAFgMiK2AJNp3cwaaslkl3QT8BHgCYCI+N+IuAbsASbSyyaAT1cVpJn1rkjL/j7gCvBPkl6W9I/p1s3rIuIyQFqurTBOM+tRkWS/Afgg8HhE3An8D8so2SXtlzQlaerK1dkuwzSzXhVJ9mlgOiJOpPVv0Ur+NyStB0jLmfneHBHjETEWEWOja0bKiNnMurBkskfEz4CLkt6fNu0CXgWOAHvTtr3A4UoiNLNSFB0u++fAU5JWAueBz9P6onhG0j7gdeC+akI0szIUSvaIOAmMzfPUrnLDMbOqeASdWSac7GaZcLKbZcLJbpYJJ7tZJpzsZplwsptlQhFR386kK8B/AbcAP69tx/NrQgzgODo5jt+03Dh+PyJG53ui1mT//51KUxEx3yCdrGJwHI6jzjhcxptlwslulol+Jft4n/bbrgkxgOPo5Dh+U2lx9KXPbmb1cxlvlolak13SbkmvSTonqbbZaCU9KWlG0qm2bbVPhS1pk6Tn03TcpyU92I9YJN0o6QVJr6Q4vpq23yrpRIrj6TR/QeUkjaT5DY/2Kw5JFyT9SNJJSVNpWz8+I5VN215bsksaAf4B+CPgduB+SbfXtPuvA7s7tvVjKuzrwJci4jZgO/BA+h3UHcuvgJ0RcQewFdgtaTvwCPBoiuNNYF/Fccx5kNb05HP6FcdHI2Jr26WufnxGqpu2PSJq+QE+DDzXtn4QOFjj/jcDp9rWXwPWp8frgdfqiqUthsPA3f2MBfht4D+AD9EavHHDfMerwv1vTB/gncBRQH2K4wJwS8e2Wo8LcBPwn6RzaWXHUWcZvwG42LY+nbb1S1+nwpa0GbgTONGPWFLpfJLWRKHHgJ8C1yLienpJXcfnMeDLwK/T+po+xRHA9yW9JGl/2lb3cal02vY6k13zbMvyUoCk9wLfBr4QEb/sRwwRMRsRW2m1rNuA2+Z7WZUxSPokMBMRL7VvrjuOZEdEfJBWN/MBSR+pYZ+depq2fSl1Jvs0sKltfSNwqcb9dyo0FXbZJK2glehPRcR3+hkLQLTu7nOc1jmEVZLm5iWs4/jsAD4l6QJwiFYp/1gf4iAiLqXlDPAsrS/Auo9LT9O2L6XOZH8R2JLOtK4EPkNrOup+qX0qbEmidRutMxHxtX7FImlU0qr0+N3Ax2idCHoeuLeuOCLiYERsjIjNtD4PP4iIz9Udh6T3SPqducfAx4FT1Hxcoupp26s+8dFxouEe4Ce0+od/VeN+vwFcBt6m9e25j1bfcBI4m5ara4jjD2mVpD8ETqafe+qOBfgA8HKK4xTw12n7+4AXgHPAN4HfqvEY3QUc7UccaX+vpJ/Tc5/NPn1GtgJT6dj8C3BzWXF4BJ1ZJjyCziwTTnazTDjZzTLhZDfLhJPdLBNOdrNMONnNMuFkN8vE/wFPa3iDy3jRJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(256, 64) dtype=float32, numpy=\n",
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.L2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
