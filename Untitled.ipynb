{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from bspnet2D import BspNet2D\n",
    "from loss_func import stage1_loss, rec_loss, stage2_loss\n",
    "from decoder2D import decoder2DStage2\n",
    "batchsize = 32\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['pixels']>\n",
      "Keys: <KeysViewHDF5 ['pixels']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('complex_elements.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    \n",
    "    \n",
    "with h5py.File('complex_elements_test.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    test_data = list(f[a_group_key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2000\n",
      "Testing Dataset Length: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Length: %d\" % len(data))\n",
    "print(\"Testing Dataset Length: %d\" % len(test_data))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batchsize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BspNet2D(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, _ = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage1_loss(output, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def test(net, test_ds):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        S,_ = net(example)\n",
    "        loss = stage1_loss(S, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "    return total_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(169.93625, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 142.112, Test_Loss: 159.143\n",
      "Epoch 001: Train_Loss: 139.721, Test_Loss: 158.781\n",
      "Epoch 002: Train_Loss: 139.113, Test_Loss: 158.571\n",
      "Epoch 003: Train_Loss: 138.658, Test_Loss: 158.388\n",
      "Epoch 004: Train_Loss: 138.249, Test_Loss: 158.169\n",
      "Epoch 005: Train_Loss: 137.876, Test_Loss: 157.994\n",
      "Epoch 006: Train_Loss: 137.554, Test_Loss: 157.851\n",
      "Epoch 007: Train_Loss: 137.241, Test_Loss: 157.643\n",
      "Epoch 008: Train_Loss: 136.928, Test_Loss: 157.438\n",
      "Epoch 009: Train_Loss: 136.647, Test_Loss: 157.267\n",
      "Epoch 010: Train_Loss: 136.388, Test_Loss: 157.086\n",
      "Epoch 011: Train_Loss: 136.138, Test_Loss: 156.937\n",
      "Epoch 012: Train_Loss: 135.896, Test_Loss: 156.755\n",
      "Epoch 013: Train_Loss: 135.644, Test_Loss: 156.601\n",
      "Epoch 014: Train_Loss: 135.406, Test_Loss: 156.465\n",
      "Epoch 015: Train_Loss: 135.167, Test_Loss: 156.291\n",
      "Epoch 016: Train_Loss: 134.944, Test_Loss: 156.156\n",
      "Epoch 017: Train_Loss: 134.721, Test_Loss: 156.020\n",
      "Epoch 018: Train_Loss: 134.501, Test_Loss: 155.876\n",
      "Epoch 019: Train_Loss: 134.290, Test_Loss: 155.763\n",
      "Epoch 020: Train_Loss: 134.079, Test_Loss: 155.624\n",
      "Epoch 021: Train_Loss: 133.886, Test_Loss: 155.511\n",
      "Epoch 022: Train_Loss: 133.680, Test_Loss: 155.396\n",
      "Epoch 023: Train_Loss: 133.493, Test_Loss: 155.270\n",
      "Epoch 024: Train_Loss: 133.195, Test_Loss: 154.601\n",
      "Epoch 025: Train_Loss: 132.486, Test_Loss: 154.071\n",
      "Epoch 026: Train_Loss: 132.017, Test_Loss: 153.706\n",
      "Epoch 027: Train_Loss: 131.658, Test_Loss: 153.397\n",
      "Epoch 028: Train_Loss: 131.368, Test_Loss: 153.142\n",
      "Epoch 029: Train_Loss: 131.056, Test_Loss: 152.925\n",
      "Epoch 030: Train_Loss: 130.799, Test_Loss: 152.721\n",
      "Epoch 031: Train_Loss: 130.550, Test_Loss: 152.523\n",
      "Epoch 032: Train_Loss: 130.313, Test_Loss: 152.351\n",
      "Epoch 033: Train_Loss: 130.085, Test_Loss: 152.175\n",
      "Epoch 034: Train_Loss: 129.856, Test_Loss: 152.024\n",
      "Epoch 035: Train_Loss: 129.641, Test_Loss: 151.864\n",
      "Epoch 036: Train_Loss: 129.424, Test_Loss: 151.713\n",
      "Epoch 037: Train_Loss: 129.232, Test_Loss: 151.564\n",
      "Epoch 038: Train_Loss: 129.025, Test_Loss: 151.409\n",
      "Epoch 039: Train_Loss: 128.835, Test_Loss: 151.281\n",
      "Epoch 040: Train_Loss: 128.630, Test_Loss: 151.161\n",
      "Epoch 041: Train_Loss: 128.442, Test_Loss: 151.072\n",
      "Epoch 042: Train_Loss: 128.255, Test_Loss: 150.952\n",
      "Epoch 043: Train_Loss: 128.056, Test_Loss: 150.824\n",
      "Epoch 044: Train_Loss: 127.875, Test_Loss: 150.710\n",
      "Epoch 045: Train_Loss: 127.698, Test_Loss: 150.576\n",
      "Epoch 046: Train_Loss: 127.506, Test_Loss: 150.444\n",
      "Epoch 047: Train_Loss: 127.330, Test_Loss: 150.304\n",
      "Epoch 048: Train_Loss: 127.141, Test_Loss: 150.189\n",
      "Epoch 049: Train_Loss: 126.967, Test_Loss: 150.037\n",
      "Epoch 050: Train_Loss: 126.785, Test_Loss: 149.908\n",
      "Epoch 051: Train_Loss: 126.605, Test_Loss: 149.768\n",
      "Epoch 052: Train_Loss: 126.432, Test_Loss: 149.679\n",
      "Epoch 053: Train_Loss: 126.260, Test_Loss: 149.545\n",
      "Epoch 054: Train_Loss: 126.092, Test_Loss: 149.437\n",
      "Epoch 055: Train_Loss: 125.930, Test_Loss: 149.312\n",
      "Epoch 056: Train_Loss: 125.773, Test_Loss: 149.178\n",
      "Epoch 057: Train_Loss: 125.596, Test_Loss: 149.062\n",
      "Epoch 058: Train_Loss: 125.444, Test_Loss: 148.936\n",
      "Epoch 059: Train_Loss: 125.281, Test_Loss: 148.815\n",
      "Epoch 060: Train_Loss: 125.107, Test_Loss: 148.743\n",
      "Epoch 061: Train_Loss: 124.941, Test_Loss: 148.639\n",
      "Epoch 062: Train_Loss: 124.769, Test_Loss: 148.566\n",
      "Epoch 063: Train_Loss: 124.612, Test_Loss: 148.466\n",
      "Epoch 064: Train_Loss: 124.467, Test_Loss: 148.370\n",
      "Epoch 065: Train_Loss: 124.311, Test_Loss: 148.249\n",
      "Epoch 066: Train_Loss: 124.155, Test_Loss: 148.180\n",
      "Epoch 067: Train_Loss: 123.999, Test_Loss: 148.105\n",
      "Epoch 068: Train_Loss: 123.848, Test_Loss: 148.020\n",
      "Epoch 069: Train_Loss: 123.688, Test_Loss: 147.922\n",
      "Epoch 070: Train_Loss: 123.535, Test_Loss: 147.837\n",
      "Epoch 071: Train_Loss: 123.382, Test_Loss: 147.742\n",
      "Epoch 072: Train_Loss: 123.233, Test_Loss: 147.673\n",
      "Epoch 073: Train_Loss: 123.089, Test_Loss: 147.560\n",
      "Epoch 074: Train_Loss: 122.919, Test_Loss: 147.424\n",
      "Epoch 075: Train_Loss: 122.768, Test_Loss: 147.311\n",
      "Epoch 076: Train_Loss: 122.610, Test_Loss: 147.240\n",
      "Epoch 077: Train_Loss: 122.469, Test_Loss: 147.123\n",
      "Epoch 078: Train_Loss: 122.335, Test_Loss: 147.018\n",
      "Epoch 079: Train_Loss: 122.189, Test_Loss: 146.905\n",
      "Epoch 080: Train_Loss: 122.013, Test_Loss: 146.814\n",
      "Epoch 081: Train_Loss: 121.862, Test_Loss: 146.701\n",
      "Epoch 082: Train_Loss: 121.710, Test_Loss: 146.613\n",
      "Epoch 083: Train_Loss: 121.567, Test_Loss: 146.522\n",
      "Epoch 084: Train_Loss: 121.416, Test_Loss: 146.424\n",
      "Epoch 085: Train_Loss: 121.276, Test_Loss: 146.349\n",
      "Epoch 086: Train_Loss: 121.134, Test_Loss: 146.220\n",
      "Epoch 087: Train_Loss: 120.983, Test_Loss: 146.139\n",
      "Epoch 088: Train_Loss: 120.845, Test_Loss: 146.053\n",
      "Epoch 089: Train_Loss: 120.688, Test_Loss: 145.952\n",
      "Epoch 090: Train_Loss: 120.536, Test_Loss: 145.900\n",
      "Epoch 091: Train_Loss: 120.380, Test_Loss: 145.812\n",
      "Epoch 092: Train_Loss: 120.235, Test_Loss: 145.726\n",
      "Epoch 093: Train_Loss: 120.096, Test_Loss: 145.620\n",
      "Epoch 094: Train_Loss: 119.958, Test_Loss: 145.518\n",
      "Epoch 095: Train_Loss: 119.826, Test_Loss: 145.427\n",
      "Epoch 096: Train_Loss: 119.683, Test_Loss: 145.292\n",
      "Epoch 097: Train_Loss: 119.558, Test_Loss: 145.172\n",
      "Epoch 098: Train_Loss: 119.430, Test_Loss: 145.072\n",
      "Epoch 099: Train_Loss: 119.277, Test_Loss: 144.982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 100\n",
    "net.load_weights('stage1_version2_BS32')\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step(net, example, opt)\n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "        \n",
    "    test_loss = test(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss, test_loss))\n",
    "    train_loss_list.append(total_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('stage1_version2_BS32')\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(110.118835, shape=(), dtype=float32)\n",
      "(1, 4096, 64)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "net.decoder.switchStage(1)\n",
    "net.load_weights('stage1_version2_BS32')\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "em = tf.constant(test_data[11].reshape(1,64,64,1))\n",
    "ps, x2 = net(tf.cast(em,dtype=tf.float32))\n",
    "print(x2.shape)\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[11].reshape(64,64)\n",
    "ps[ps>1]=1\n",
    "ps[ps<=0.01]=0\n",
    "print(ps)\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda3c08d690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda300a9950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.switchStage(2)\n",
    "net.decoder.stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(256, 64) dtype=float32, numpy=\n",
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.L2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_stage2(net, example, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, M = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage2_loss(output, F,M) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)\n",
    "\n",
    "def test_stage2(net, test_ds):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        output, M = net(example)\n",
    "        loss = stage2_loss(output, F,M) \n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "    return total_loss\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12831.43, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 1790.610, Test_Loss: 939.880\n",
      "model saved\n",
      "Epoch 001: Train_Loss: 989.951, Test_Loss: 964.504\n",
      "Epoch 002: Train_Loss: 957.741, Test_Loss: 899.658\n",
      "model saved\n",
      "Epoch 003: Train_Loss: 904.005, Test_Loss: 872.873\n",
      "model saved\n",
      "Epoch 004: Train_Loss: 857.626, Test_Loss: 851.916\n",
      "model saved\n",
      "Epoch 005: Train_Loss: 873.406, Test_Loss: 858.512\n",
      "Epoch 006: Train_Loss: 833.190, Test_Loss: 825.118\n",
      "model saved\n",
      "Epoch 007: Train_Loss: 825.347, Test_Loss: 822.666\n",
      "model saved\n",
      "Epoch 008: Train_Loss: 818.575, Test_Loss: 811.811\n",
      "model saved\n",
      "Epoch 009: Train_Loss: 799.387, Test_Loss: 782.775\n",
      "model saved\n",
      "Epoch 010: Train_Loss: 779.805, Test_Loss: 745.974\n",
      "model saved\n",
      "Epoch 011: Train_Loss: 718.057, Test_Loss: 711.312\n",
      "model saved\n",
      "Epoch 012: Train_Loss: 693.623, Test_Loss: 695.718\n",
      "model saved\n",
      "Epoch 013: Train_Loss: 671.571, Test_Loss: 638.328\n",
      "model saved\n",
      "Epoch 014: Train_Loss: 639.474, Test_Loss: 626.704\n",
      "model saved\n",
      "Epoch 015: Train_Loss: 621.707, Test_Loss: 616.546\n",
      "model saved\n",
      "Epoch 016: Train_Loss: 623.293, Test_Loss: 602.142\n",
      "model saved\n",
      "Epoch 017: Train_Loss: 594.956, Test_Loss: 583.351\n",
      "model saved\n",
      "Epoch 018: Train_Loss: 586.636, Test_Loss: 584.824\n",
      "Epoch 019: Train_Loss: 580.534, Test_Loss: 596.099\n",
      "Epoch 020: Train_Loss: 586.510, Test_Loss: 573.573\n",
      "model saved\n",
      "Epoch 021: Train_Loss: 593.464, Test_Loss: 575.999\n",
      "Epoch 022: Train_Loss: 576.076, Test_Loss: 569.658\n",
      "model saved\n",
      "Epoch 023: Train_Loss: 555.030, Test_Loss: 557.270\n",
      "model saved\n",
      "Epoch 024: Train_Loss: 560.482, Test_Loss: 554.342\n",
      "model saved\n",
      "Epoch 025: Train_Loss: 559.671, Test_Loss: 560.035\n",
      "Epoch 026: Train_Loss: 571.510, Test_Loss: 574.109\n",
      "Epoch 027: Train_Loss: 550.527, Test_Loss: 541.251\n",
      "model saved\n",
      "Epoch 028: Train_Loss: 543.061, Test_Loss: 541.899\n",
      "Epoch 029: Train_Loss: 546.152, Test_Loss: 548.805\n",
      "Epoch 030: Train_Loss: 533.973, Test_Loss: 529.008\n",
      "model saved\n",
      "Epoch 031: Train_Loss: 508.999, Test_Loss: 505.951\n",
      "model saved\n",
      "Epoch 032: Train_Loss: 507.507, Test_Loss: 523.501\n",
      "Epoch 033: Train_Loss: 506.317, Test_Loss: 503.682\n",
      "model saved\n",
      "Epoch 034: Train_Loss: 486.376, Test_Loss: 517.804\n",
      "Epoch 035: Train_Loss: 520.938, Test_Loss: 520.024\n",
      "Epoch 036: Train_Loss: 498.654, Test_Loss: 526.010\n",
      "Epoch 037: Train_Loss: 514.318, Test_Loss: 500.177\n",
      "model saved\n",
      "Epoch 038: Train_Loss: 517.560, Test_Loss: 559.722\n",
      "Epoch 039: Train_Loss: 510.137, Test_Loss: 523.096\n",
      "Epoch 040: Train_Loss: 480.711, Test_Loss: 511.717\n",
      "Epoch 041: Train_Loss: 494.815, Test_Loss: 502.881\n",
      "Epoch 042: Train_Loss: 479.788, Test_Loss: 482.946\n",
      "model saved\n",
      "Epoch 043: Train_Loss: 487.730, Test_Loss: 526.235\n",
      "Epoch 044: Train_Loss: 497.747, Test_Loss: 496.894\n",
      "Epoch 045: Train_Loss: 475.176, Test_Loss: 507.337\n",
      "Epoch 046: Train_Loss: 487.622, Test_Loss: 510.496\n",
      "Epoch 047: Train_Loss: 518.264, Test_Loss: 548.786\n",
      "Epoch 048: Train_Loss: 491.984, Test_Loss: 494.290\n",
      "Epoch 049: Train_Loss: 479.512, Test_Loss: 503.286\n",
      "Epoch 050: Train_Loss: 490.355, Test_Loss: 491.759\n",
      "Epoch 051: Train_Loss: 478.307, Test_Loss: 490.269\n",
      "Epoch 052: Train_Loss: 507.816, Test_Loss: 480.628\n",
      "model saved\n",
      "Epoch 053: Train_Loss: 465.766, Test_Loss: 466.812\n",
      "model saved\n",
      "Epoch 054: Train_Loss: 465.261, Test_Loss: 511.431\n",
      "Epoch 055: Train_Loss: 477.337, Test_Loss: 485.601\n",
      "Epoch 056: Train_Loss: 482.068, Test_Loss: 488.545\n",
      "Epoch 057: Train_Loss: 492.244, Test_Loss: 481.957\n",
      "Epoch 058: Train_Loss: 468.942, Test_Loss: 498.500\n",
      "Epoch 059: Train_Loss: 486.176, Test_Loss: 501.027\n",
      "Epoch 060: Train_Loss: 497.064, Test_Loss: 510.327\n",
      "Epoch 061: Train_Loss: 483.949, Test_Loss: 507.170\n",
      "Epoch 062: Train_Loss: 471.739, Test_Loss: 491.859\n",
      "Epoch 063: Train_Loss: 458.697, Test_Loss: 488.744\n",
      "Epoch 064: Train_Loss: 464.671, Test_Loss: 480.902\n",
      "Epoch 065: Train_Loss: 474.083, Test_Loss: 482.615\n",
      "Epoch 066: Train_Loss: 455.879, Test_Loss: 478.883\n",
      "Epoch 067: Train_Loss: 477.994, Test_Loss: 511.606\n",
      "Epoch 068: Train_Loss: 485.187, Test_Loss: 486.797\n",
      "Epoch 069: Train_Loss: 467.292, Test_Loss: 512.134\n",
      "Epoch 070: Train_Loss: 479.422, Test_Loss: 508.909\n",
      "Epoch 071: Train_Loss: 484.951, Test_Loss: 495.639\n",
      "Epoch 072: Train_Loss: 483.854, Test_Loss: 492.647\n",
      "Epoch 073: Train_Loss: 479.826, Test_Loss: 466.835\n",
      "Epoch 074: Train_Loss: 483.809, Test_Loss: 463.964\n",
      "model saved\n",
      "Epoch 075: Train_Loss: 478.699, Test_Loss: 491.527\n",
      "Epoch 076: Train_Loss: 484.550, Test_Loss: 495.194\n",
      "Epoch 077: Train_Loss: 478.905, Test_Loss: 499.431\n",
      "Epoch 078: Train_Loss: 468.093, Test_Loss: 493.460\n",
      "Epoch 079: Train_Loss: 471.532, Test_Loss: 518.808\n",
      "Epoch 080: Train_Loss: 479.017, Test_Loss: 521.780\n",
      "Epoch 081: Train_Loss: 494.545, Test_Loss: 519.703\n",
      "Epoch 082: Train_Loss: 478.142, Test_Loss: 490.228\n",
      "Epoch 083: Train_Loss: 471.865, Test_Loss: 553.295\n",
      "Epoch 084: Train_Loss: 469.001, Test_Loss: 488.851\n",
      "Epoch 085: Train_Loss: 456.655, Test_Loss: 498.104\n",
      "Epoch 086: Train_Loss: 461.778, Test_Loss: 521.485\n",
      "Epoch 087: Train_Loss: 481.245, Test_Loss: 517.346\n",
      "Epoch 088: Train_Loss: 468.495, Test_Loss: 492.468\n",
      "Epoch 089: Train_Loss: 465.809, Test_Loss: 512.736\n",
      "Epoch 090: Train_Loss: 462.032, Test_Loss: 487.806\n",
      "Epoch 091: Train_Loss: 462.809, Test_Loss: 474.302\n",
      "Epoch 092: Train_Loss: 461.286, Test_Loss: 490.071\n",
      "Epoch 093: Train_Loss: 475.908, Test_Loss: 472.759\n",
      "Epoch 094: Train_Loss: 484.941, Test_Loss: 493.625\n",
      "Epoch 095: Train_Loss: 469.583, Test_Loss: 495.970\n",
      "Epoch 096: Train_Loss: 477.490, Test_Loss: 486.345\n",
      "Epoch 097: Train_Loss: 476.257, Test_Loss: 503.384\n",
      "Epoch 098: Train_Loss: 463.044, Test_Loss: 479.556\n",
      "Epoch 099: Train_Loss: 437.959, Test_Loss: 457.790\n",
      "model saved\n",
      "Epoch 100: Train_Loss: 441.639, Test_Loss: 466.446\n",
      "Epoch 101: Train_Loss: 465.243, Test_Loss: 510.985\n",
      "Epoch 102: Train_Loss: 475.567, Test_Loss: 478.002\n",
      "Epoch 103: Train_Loss: 479.298, Test_Loss: 467.595\n",
      "Epoch 104: Train_Loss: 478.728, Test_Loss: 460.072\n",
      "Epoch 105: Train_Loss: 454.418, Test_Loss: 464.165\n",
      "Epoch 106: Train_Loss: 437.950, Test_Loss: 466.337\n",
      "Epoch 107: Train_Loss: 448.386, Test_Loss: 491.744\n",
      "Epoch 108: Train_Loss: 486.172, Test_Loss: 487.314\n",
      "Epoch 109: Train_Loss: 477.959, Test_Loss: 481.583\n",
      "Epoch 110: Train_Loss: 481.779, Test_Loss: 493.129\n",
      "Epoch 111: Train_Loss: 466.779, Test_Loss: 484.569\n",
      "Epoch 112: Train_Loss: 480.794, Test_Loss: 491.916\n",
      "Epoch 113: Train_Loss: 482.563, Test_Loss: 479.199\n",
      "Epoch 114: Train_Loss: 480.125, Test_Loss: 499.805\n",
      "Epoch 115: Train_Loss: 479.306, Test_Loss: 493.855\n",
      "Epoch 116: Train_Loss: 452.819, Test_Loss: 472.110\n",
      "Epoch 117: Train_Loss: 434.021, Test_Loss: 475.092\n",
      "Epoch 118: Train_Loss: 445.781, Test_Loss: 467.400\n",
      "Epoch 119: Train_Loss: 475.149, Test_Loss: 500.852\n",
      "Epoch 120: Train_Loss: 481.986, Test_Loss: 494.703\n",
      "Epoch 121: Train_Loss: 485.054, Test_Loss: 503.289\n",
      "Epoch 122: Train_Loss: 502.965, Test_Loss: 499.000\n",
      "Epoch 123: Train_Loss: 520.613, Test_Loss: 466.645\n",
      "Epoch 124: Train_Loss: 435.162, Test_Loss: 449.343\n",
      "model saved\n",
      "Epoch 125: Train_Loss: 425.464, Test_Loss: 478.655\n",
      "Epoch 126: Train_Loss: 431.553, Test_Loss: 454.230\n",
      "Epoch 127: Train_Loss: 428.879, Test_Loss: 462.506\n",
      "Epoch 128: Train_Loss: 434.939, Test_Loss: 465.963\n",
      "Epoch 129: Train_Loss: 470.011, Test_Loss: 491.213\n",
      "Epoch 130: Train_Loss: 467.240, Test_Loss: 463.680\n",
      "Epoch 131: Train_Loss: 459.672, Test_Loss: 475.267\n",
      "Epoch 132: Train_Loss: 469.078, Test_Loss: 500.964\n",
      "Epoch 133: Train_Loss: 476.302, Test_Loss: 477.851\n",
      "Epoch 134: Train_Loss: 473.213, Test_Loss: 477.247\n",
      "Epoch 135: Train_Loss: 470.723, Test_Loss: 490.531\n",
      "Epoch 136: Train_Loss: 469.322, Test_Loss: 483.832\n",
      "Epoch 137: Train_Loss: 459.435, Test_Loss: 496.513\n",
      "Epoch 138: Train_Loss: 461.581, Test_Loss: 500.411\n",
      "Epoch 139: Train_Loss: 449.650, Test_Loss: 469.128\n",
      "Epoch 140: Train_Loss: 457.770, Test_Loss: 504.815\n",
      "Epoch 141: Train_Loss: 446.360, Test_Loss: 495.531\n",
      "Epoch 142: Train_Loss: 440.975, Test_Loss: 495.013\n",
      "Epoch 143: Train_Loss: 446.751, Test_Loss: 531.526\n",
      "Epoch 144: Train_Loss: 461.055, Test_Loss: 534.688\n",
      "Epoch 145: Train_Loss: 460.487, Test_Loss: 538.818\n",
      "Epoch 146: Train_Loss: 461.349, Test_Loss: 534.548\n",
      "Epoch 147: Train_Loss: 475.984, Test_Loss: 500.115\n",
      "Epoch 148: Train_Loss: 445.900, Test_Loss: 513.243\n",
      "Epoch 149: Train_Loss: 447.199, Test_Loss: 514.490\n",
      "Epoch 150: Train_Loss: 471.598, Test_Loss: 547.584\n",
      "Epoch 151: Train_Loss: 466.020, Test_Loss: 504.934\n",
      "Epoch 152: Train_Loss: 485.740, Test_Loss: 506.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153: Train_Loss: 491.476, Test_Loss: 496.355\n",
      "Epoch 154: Train_Loss: 484.159, Test_Loss: 503.503\n",
      "Epoch 155: Train_Loss: 458.412, Test_Loss: 484.708\n",
      "Epoch 156: Train_Loss: 464.111, Test_Loss: 493.716\n",
      "Epoch 157: Train_Loss: 488.433, Test_Loss: 487.441\n",
      "Epoch 158: Train_Loss: 490.813, Test_Loss: 504.304\n",
      "Epoch 159: Train_Loss: 484.319, Test_Loss: 507.532\n",
      "Epoch 160: Train_Loss: 466.636, Test_Loss: 491.391\n",
      "Epoch 161: Train_Loss: 449.616, Test_Loss: 471.371\n",
      "Epoch 162: Train_Loss: 438.092, Test_Loss: 475.807\n",
      "Epoch 163: Train_Loss: 432.942, Test_Loss: 470.930\n",
      "Epoch 164: Train_Loss: 436.814, Test_Loss: 470.422\n",
      "Epoch 165: Train_Loss: 435.796, Test_Loss: 485.842\n",
      "Epoch 166: Train_Loss: 449.385, Test_Loss: 504.102\n",
      "Epoch 167: Train_Loss: 470.344, Test_Loss: 501.983\n",
      "Epoch 168: Train_Loss: 463.042, Test_Loss: 478.325\n",
      "Epoch 169: Train_Loss: 460.229, Test_Loss: 495.498\n",
      "Epoch 170: Train_Loss: 460.704, Test_Loss: 512.636\n",
      "Epoch 171: Train_Loss: 457.133, Test_Loss: 505.849\n",
      "Epoch 172: Train_Loss: 461.208, Test_Loss: 497.335\n",
      "Epoch 173: Train_Loss: 461.123, Test_Loss: 537.124\n",
      "Epoch 174: Train_Loss: 477.472, Test_Loss: 537.198\n",
      "Epoch 175: Train_Loss: 468.596, Test_Loss: 535.674\n",
      "Epoch 176: Train_Loss: 473.359, Test_Loss: 514.353\n",
      "Epoch 177: Train_Loss: 471.620, Test_Loss: 506.992\n",
      "Epoch 178: Train_Loss: 479.874, Test_Loss: 527.891\n",
      "Epoch 179: Train_Loss: 477.151, Test_Loss: 528.856\n",
      "Epoch 180: Train_Loss: 480.753, Test_Loss: 507.921\n",
      "Epoch 181: Train_Loss: 467.709, Test_Loss: 503.567\n",
      "Epoch 182: Train_Loss: 457.541, Test_Loss: 479.763\n",
      "Epoch 183: Train_Loss: 455.617, Test_Loss: 472.950\n",
      "Epoch 184: Train_Loss: 453.935, Test_Loss: 460.978\n",
      "Epoch 185: Train_Loss: 454.449, Test_Loss: 494.469\n",
      "Epoch 186: Train_Loss: 454.463, Test_Loss: 522.401\n",
      "Epoch 187: Train_Loss: 448.434, Test_Loss: 538.592\n",
      "Epoch 188: Train_Loss: 449.590, Test_Loss: 533.070\n",
      "Epoch 189: Train_Loss: 457.155, Test_Loss: 541.707\n",
      "Epoch 190: Train_Loss: 463.706, Test_Loss: 515.916\n",
      "Epoch 191: Train_Loss: 462.608, Test_Loss: 492.664\n",
      "Epoch 192: Train_Loss: 443.425, Test_Loss: 475.189\n",
      "Epoch 193: Train_Loss: 454.869, Test_Loss: 465.359\n",
      "Epoch 194: Train_Loss: 461.244, Test_Loss: 455.098\n",
      "Epoch 195: Train_Loss: 451.801, Test_Loss: 458.191\n",
      "Epoch 196: Train_Loss: 467.697, Test_Loss: 456.283\n",
      "Epoch 197: Train_Loss: 453.672, Test_Loss: 459.218\n",
      "Epoch 198: Train_Loss: 456.424, Test_Loss: 471.805\n",
      "Epoch 199: Train_Loss: 453.134, Test_Loss: 473.809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "epoch = 200\n",
    "net.load_weights('stage2_version0_BS32')\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "prev_test_loss = test_stage2(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step_stage2(net, example, opt)\n",
    "        total_loss = (total_loss*counter + loss)/(counter + 1)\n",
    "        counter += 1\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "        \n",
    "    test_loss = test_stage2(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss, test_loss))\n",
    "    train_loss_list.append(total_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('stage2_version0_BS32')\n",
    "        print(\"model saved\")\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "1.0000001\n",
      "tf.Tensor(\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]], shape=(1, 4096, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "net.decoder.switchStage(2)\n",
    "net.load_weights('stage2_version0_BS32')\n",
    "prev_test_loss = test_stage2(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "em = tf.constant(test_data[11].reshape(1,64,64,1))\n",
    "\n",
    "ps, x2 = net(tf.cast(em,dtype=tf.float32))\n",
    "print(net.decoder.L3.weight.numpy().max())\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[11].reshape(64,64)\n",
    "print(x2)\n",
    "# ps[ps>0]=1\n",
    "# ps[ps<=0]=0\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36e80c7850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhe/anaconda3/lib/python3.7/site-packages/matplotlib/image.py:397: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "/home/lizhe/anaconda3/lib/python3.7/site-packages/matplotlib/image.py:398: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "/home/lizhe/anaconda3/lib/python3.7/site-packages/matplotlib/image.py:405: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "/home/lizhe/anaconda3/lib/python3.7/site-packages/matplotlib/image.py:410: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "<string>:6: UserWarning: Warning: converting a masked element to nan.\n",
      "/home/lizhe/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py:722: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(256, 64) dtype=float32, numpy=\n",
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.decoder.L2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
