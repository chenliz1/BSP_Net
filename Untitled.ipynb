{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from bspnet2D import BspNet2D\n",
    "from loss_func import stage1_loss, rec_loss\n",
    "batchsize = 32\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['pixels']>\n",
      "Keys: <KeysViewHDF5 ['pixels']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('complex_elements.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    \n",
    "    \n",
    "with h5py.File('complex_elements_test.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    test_data = list(f[a_group_key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2000\n",
      "Testing Dataset Length: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Length: %d\" % len(data))\n",
    "print(\"Testing Dataset Length: %d\" % len(test_data))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batchsize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BspNet2D(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage1_loss(output, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def test(net, test_ds):\n",
    "    total_loss = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        S = net(example)\n",
    "        loss = stage1_loss(S, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "        total_loss += loss\n",
    "    return total_loss/len(test_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.05454742, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 5.927, Test_Loss: 0.055\n",
      "Epoch 001: Train_Loss: 5.923, Test_Loss: 0.055\n",
      "Epoch 002: Train_Loss: 5.919, Test_Loss: 0.054\n",
      "Epoch 003: Train_Loss: 5.915, Test_Loss: 0.054\n",
      "Epoch 004: Train_Loss: 5.911, Test_Loss: 0.054\n",
      "Epoch 005: Train_Loss: 5.907, Test_Loss: 0.054\n",
      "Epoch 006: Train_Loss: 5.904, Test_Loss: 0.054\n",
      "Epoch 007: Train_Loss: 5.900, Test_Loss: 0.054\n",
      "Epoch 008: Train_Loss: 5.896, Test_Loss: 0.054\n",
      "Epoch 009: Train_Loss: 5.892, Test_Loss: 0.054\n",
      "Epoch 010: Train_Loss: 5.889, Test_Loss: 0.054\n",
      "Epoch 011: Train_Loss: 5.885, Test_Loss: 0.054\n",
      "Epoch 012: Train_Loss: 5.881, Test_Loss: 0.054\n",
      "Epoch 013: Train_Loss: 5.877, Test_Loss: 0.054\n",
      "Epoch 014: Train_Loss: 5.874, Test_Loss: 0.054\n",
      "Epoch 015: Train_Loss: 5.870, Test_Loss: 0.054\n",
      "Epoch 016: Train_Loss: 5.866, Test_Loss: 0.054\n",
      "Epoch 017: Train_Loss: 5.862, Test_Loss: 0.054\n",
      "Epoch 018: Train_Loss: 5.859, Test_Loss: 0.054\n",
      "Epoch 019: Train_Loss: 5.855, Test_Loss: 0.054\n",
      "Epoch 020: Train_Loss: 5.851, Test_Loss: 0.054\n",
      "Epoch 021: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 022: Train_Loss: 5.843, Test_Loss: 0.054\n",
      "Epoch 023: Train_Loss: 5.839, Test_Loss: 0.054\n",
      "Epoch 024: Train_Loss: 5.836, Test_Loss: 0.054\n",
      "Epoch 025: Train_Loss: 5.832, Test_Loss: 0.054\n",
      "Epoch 026: Train_Loss: 5.828, Test_Loss: 0.054\n",
      "Epoch 027: Train_Loss: 5.824, Test_Loss: 0.054\n",
      "Epoch 028: Train_Loss: 5.821, Test_Loss: 0.054\n",
      "Epoch 029: Train_Loss: 5.817, Test_Loss: 0.054\n",
      "Epoch 030: Train_Loss: 5.814, Test_Loss: 0.054\n",
      "Epoch 031: Train_Loss: 5.810, Test_Loss: 0.054\n",
      "Epoch 032: Train_Loss: 5.807, Test_Loss: 0.054\n",
      "Epoch 033: Train_Loss: 5.804, Test_Loss: 0.054\n",
      "Epoch 034: Train_Loss: 5.802, Test_Loss: 0.054\n",
      "Epoch 035: Train_Loss: 5.800, Test_Loss: 0.054\n",
      "Epoch 036: Train_Loss: 5.799, Test_Loss: 0.054\n",
      "Epoch 037: Train_Loss: 5.799, Test_Loss: 0.054\n",
      "Epoch 038: Train_Loss: 5.803, Test_Loss: 0.054\n",
      "Epoch 039: Train_Loss: 5.815, Test_Loss: 0.055\n",
      "Epoch 040: Train_Loss: 5.844, Test_Loss: 0.055\n",
      "Epoch 041: Train_Loss: 5.901, Test_Loss: 0.056\n",
      "Epoch 042: Train_Loss: 5.973, Test_Loss: 0.056\n",
      "Epoch 043: Train_Loss: 5.966, Test_Loss: 0.058\n",
      "Epoch 044: Train_Loss: 5.982, Test_Loss: 0.057\n",
      "Epoch 045: Train_Loss: 6.055, Test_Loss: 0.058\n",
      "Epoch 046: Train_Loss: 6.016, Test_Loss: 0.055\n",
      "Epoch 047: Train_Loss: 5.888, Test_Loss: 0.055\n",
      "Epoch 048: Train_Loss: 5.857, Test_Loss: 0.054\n",
      "Epoch 049: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 050: Train_Loss: 5.844, Test_Loss: 0.054\n",
      "Epoch 051: Train_Loss: 5.842, Test_Loss: 0.054\n",
      "Epoch 052: Train_Loss: 5.841, Test_Loss: 0.054\n",
      "Epoch 053: Train_Loss: 5.842, Test_Loss: 0.054\n",
      "Epoch 054: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 055: Train_Loss: 5.855, Test_Loss: 0.054\n",
      "Epoch 056: Train_Loss: 5.867, Test_Loss: 0.055\n",
      "Epoch 057: Train_Loss: 5.888, Test_Loss: 0.055\n",
      "Epoch 058: Train_Loss: 5.928, Test_Loss: 0.057\n",
      "Epoch 059: Train_Loss: 6.009, Test_Loss: 0.059\n",
      "Epoch 060: Train_Loss: 6.184, Test_Loss: 0.058\n",
      "Epoch 061: Train_Loss: 6.383, Test_Loss: 0.055\n",
      "Epoch 062: Train_Loss: 6.198, Test_Loss: 0.056\n",
      "Epoch 063: Train_Loss: 5.997, Test_Loss: 0.055\n",
      "Epoch 064: Train_Loss: 5.912, Test_Loss: 0.055\n",
      "Epoch 065: Train_Loss: 5.871, Test_Loss: 0.055\n",
      "Epoch 066: Train_Loss: 5.846, Test_Loss: 0.054\n",
      "Epoch 067: Train_Loss: 5.828, Test_Loss: 0.054\n",
      "Epoch 068: Train_Loss: 5.813, Test_Loss: 0.054\n",
      "Epoch 069: Train_Loss: 5.801, Test_Loss: 0.054\n",
      "Epoch 070: Train_Loss: 5.791, Test_Loss: 0.054\n",
      "Epoch 071: Train_Loss: 5.783, Test_Loss: 0.054\n",
      "Epoch 072: Train_Loss: 5.776, Test_Loss: 0.054\n",
      "Epoch 073: Train_Loss: 5.770, Test_Loss: 0.054\n",
      "Epoch 074: Train_Loss: 5.764, Test_Loss: 0.054\n",
      "Epoch 075: Train_Loss: 5.759, Test_Loss: 0.054\n",
      "Epoch 076: Train_Loss: 5.754, Test_Loss: 0.054\n",
      "Epoch 077: Train_Loss: 5.750, Test_Loss: 0.054\n",
      "Epoch 078: Train_Loss: 5.746, Test_Loss: 0.054\n",
      "Epoch 079: Train_Loss: 5.742, Test_Loss: 0.054\n",
      "Epoch 080: Train_Loss: 5.738, Test_Loss: 0.054\n",
      "Epoch 081: Train_Loss: 5.734, Test_Loss: 0.054\n",
      "Epoch 082: Train_Loss: 5.731, Test_Loss: 0.054\n",
      "Epoch 083: Train_Loss: 5.727, Test_Loss: 0.054\n",
      "Epoch 084: Train_Loss: 5.724, Test_Loss: 0.054\n",
      "Epoch 085: Train_Loss: 5.720, Test_Loss: 0.054\n",
      "Epoch 086: Train_Loss: 5.717, Test_Loss: 0.054\n",
      "Epoch 087: Train_Loss: 5.714, Test_Loss: 0.054\n",
      "Epoch 088: Train_Loss: 5.710, Test_Loss: 0.054\n",
      "Epoch 089: Train_Loss: 5.707, Test_Loss: 0.054\n",
      "Epoch 090: Train_Loss: 5.704, Test_Loss: 0.054\n",
      "Epoch 091: Train_Loss: 5.701, Test_Loss: 0.054\n",
      "Epoch 092: Train_Loss: 5.698, Test_Loss: 0.054\n",
      "Epoch 093: Train_Loss: 5.694, Test_Loss: 0.053\n",
      "Epoch 094: Train_Loss: 5.691, Test_Loss: 0.053\n",
      "Epoch 095: Train_Loss: 5.688, Test_Loss: 0.053\n",
      "Epoch 096: Train_Loss: 5.685, Test_Loss: 0.053\n",
      "Epoch 097: Train_Loss: 5.682, Test_Loss: 0.053\n",
      "Epoch 098: Train_Loss: 5.679, Test_Loss: 0.053\n",
      "Epoch 099: Train_Loss: 5.676, Test_Loss: 0.053\n",
      "Epoch 100: Train_Loss: 5.673, Test_Loss: 0.053\n",
      "Epoch 101: Train_Loss: 5.670, Test_Loss: 0.053\n",
      "Epoch 102: Train_Loss: 5.667, Test_Loss: 0.053\n",
      "Epoch 103: Train_Loss: 5.663, Test_Loss: 0.053\n",
      "Epoch 104: Train_Loss: 5.660, Test_Loss: 0.053\n",
      "Epoch 105: Train_Loss: 5.657, Test_Loss: 0.053\n",
      "Epoch 106: Train_Loss: 5.654, Test_Loss: 0.053\n",
      "Epoch 107: Train_Loss: 5.651, Test_Loss: 0.053\n",
      "Epoch 108: Train_Loss: 5.648, Test_Loss: 0.053\n",
      "Epoch 109: Train_Loss: 5.646, Test_Loss: 0.053\n",
      "Epoch 110: Train_Loss: 5.643, Test_Loss: 0.053\n",
      "Epoch 111: Train_Loss: 5.640, Test_Loss: 0.053\n",
      "Epoch 112: Train_Loss: 5.637, Test_Loss: 0.053\n",
      "Epoch 113: Train_Loss: 5.635, Test_Loss: 0.053\n",
      "Epoch 114: Train_Loss: 5.632, Test_Loss: 0.053\n",
      "Epoch 115: Train_Loss: 5.630, Test_Loss: 0.053\n",
      "Epoch 116: Train_Loss: 5.629, Test_Loss: 0.053\n",
      "Epoch 117: Train_Loss: 5.630, Test_Loss: 0.053\n",
      "Epoch 118: Train_Loss: 5.634, Test_Loss: 0.053\n",
      "Epoch 119: Train_Loss: 5.642, Test_Loss: 0.053\n",
      "Epoch 120: Train_Loss: 5.655, Test_Loss: 0.054\n",
      "Epoch 121: Train_Loss: 5.683, Test_Loss: 0.054\n",
      "Epoch 122: Train_Loss: 5.698, Test_Loss: 0.054\n",
      "Epoch 123: Train_Loss: 5.679, Test_Loss: 0.054\n",
      "Epoch 124: Train_Loss: 5.663, Test_Loss: 0.054\n",
      "Epoch 125: Train_Loss: 5.656, Test_Loss: 0.054\n",
      "Epoch 126: Train_Loss: 5.668, Test_Loss: 0.054\n",
      "Epoch 127: Train_Loss: 5.693, Test_Loss: 0.054\n",
      "Epoch 128: Train_Loss: 5.754, Test_Loss: 0.055\n",
      "Epoch 129: Train_Loss: 5.869, Test_Loss: 0.060\n",
      "Epoch 130: Train_Loss: 5.880, Test_Loss: 0.054\n",
      "Epoch 131: Train_Loss: 5.697, Test_Loss: 0.054\n",
      "Epoch 132: Train_Loss: 5.669, Test_Loss: 0.054\n",
      "Epoch 133: Train_Loss: 5.664, Test_Loss: 0.054\n",
      "Epoch 134: Train_Loss: 5.663, Test_Loss: 0.054\n",
      "Epoch 135: Train_Loss: 5.664, Test_Loss: 0.054\n",
      "Epoch 136: Train_Loss: 5.667, Test_Loss: 0.054\n",
      "Epoch 137: Train_Loss: 5.673, Test_Loss: 0.054\n",
      "Epoch 138: Train_Loss: 5.679, Test_Loss: 0.054\n",
      "Epoch 139: Train_Loss: 5.685, Test_Loss: 0.054\n",
      "Epoch 140: Train_Loss: 5.688, Test_Loss: 0.054\n",
      "Epoch 141: Train_Loss: 5.689, Test_Loss: 0.054\n",
      "Epoch 142: Train_Loss: 5.691, Test_Loss: 0.054\n",
      "Epoch 143: Train_Loss: 5.693, Test_Loss: 0.055\n",
      "Epoch 144: Train_Loss: 5.697, Test_Loss: 0.055\n",
      "Epoch 145: Train_Loss: 5.700, Test_Loss: 0.055\n",
      "Epoch 146: Train_Loss: 5.706, Test_Loss: 0.055\n",
      "Epoch 147: Train_Loss: 5.717, Test_Loss: 0.055\n",
      "Epoch 148: Train_Loss: 5.735, Test_Loss: 0.055\n",
      "Epoch 149: Train_Loss: 5.763, Test_Loss: 0.055\n",
      "Epoch 150: Train_Loss: 5.805, Test_Loss: 0.055\n",
      "Epoch 151: Train_Loss: 5.876, Test_Loss: 0.056\n",
      "Epoch 152: Train_Loss: 6.023, Test_Loss: 0.058\n",
      "Epoch 153: Train_Loss: 6.331, Test_Loss: 0.061\n",
      "Epoch 154: Train_Loss: 6.374, Test_Loss: 0.055\n",
      "Epoch 155: Train_Loss: 6.032, Test_Loss: 0.054\n",
      "Epoch 156: Train_Loss: 5.885, Test_Loss: 0.054\n",
      "Epoch 157: Train_Loss: 5.810, Test_Loss: 0.054\n",
      "Epoch 158: Train_Loss: 5.762, Test_Loss: 0.053\n",
      "Epoch 159: Train_Loss: 5.728, Test_Loss: 0.053\n",
      "Epoch 160: Train_Loss: 5.702, Test_Loss: 0.053\n",
      "Epoch 161: Train_Loss: 5.682, Test_Loss: 0.053\n",
      "Epoch 162: Train_Loss: 5.667, Test_Loss: 0.053\n",
      "Epoch 163: Train_Loss: 5.654, Test_Loss: 0.053\n",
      "Epoch 164: Train_Loss: 5.643, Test_Loss: 0.053\n",
      "Epoch 165: Train_Loss: 5.633, Test_Loss: 0.053\n",
      "Epoch 166: Train_Loss: 5.625, Test_Loss: 0.053\n",
      "Epoch 167: Train_Loss: 5.617, Test_Loss: 0.053\n",
      "Epoch 168: Train_Loss: 5.611, Test_Loss: 0.053\n",
      "Epoch 169: Train_Loss: 5.604, Test_Loss: 0.053\n",
      "Epoch 170: Train_Loss: 5.599, Test_Loss: 0.053\n",
      "Epoch 171: Train_Loss: 5.593, Test_Loss: 0.053\n",
      "Epoch 172: Train_Loss: 5.588, Test_Loss: 0.053\n",
      "Epoch 173: Train_Loss: 5.584, Test_Loss: 0.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174: Train_Loss: 5.579, Test_Loss: 0.053\n",
      "Epoch 175: Train_Loss: 5.575, Test_Loss: 0.053\n",
      "Epoch 176: Train_Loss: 5.571, Test_Loss: 0.053\n",
      "Epoch 177: Train_Loss: 5.567, Test_Loss: 0.053\n",
      "Epoch 178: Train_Loss: 5.564, Test_Loss: 0.053\n",
      "Epoch 179: Train_Loss: 5.560, Test_Loss: 0.053\n",
      "Epoch 180: Train_Loss: 5.556, Test_Loss: 0.053\n",
      "Epoch 181: Train_Loss: 5.553, Test_Loss: 0.053\n",
      "Epoch 182: Train_Loss: 5.550, Test_Loss: 0.053\n",
      "Epoch 183: Train_Loss: 5.547, Test_Loss: 0.053\n",
      "Epoch 184: Train_Loss: 5.544, Test_Loss: 0.053\n",
      "Epoch 185: Train_Loss: 5.540, Test_Loss: 0.053\n",
      "Epoch 186: Train_Loss: 5.537, Test_Loss: 0.053\n",
      "Epoch 187: Train_Loss: 5.534, Test_Loss: 0.053\n",
      "Epoch 188: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 189: Train_Loss: 5.529, Test_Loss: 0.053\n",
      "Epoch 190: Train_Loss: 5.526, Test_Loss: 0.053\n",
      "Epoch 191: Train_Loss: 5.523, Test_Loss: 0.053\n",
      "Epoch 192: Train_Loss: 5.520, Test_Loss: 0.053\n",
      "Epoch 193: Train_Loss: 5.518, Test_Loss: 0.053\n",
      "Epoch 194: Train_Loss: 5.515, Test_Loss: 0.053\n",
      "Epoch 195: Train_Loss: 5.512, Test_Loss: 0.053\n",
      "Epoch 196: Train_Loss: 5.510, Test_Loss: 0.053\n",
      "Epoch 197: Train_Loss: 5.507, Test_Loss: 0.053\n",
      "Epoch 198: Train_Loss: 5.504, Test_Loss: 0.053\n",
      "Epoch 199: Train_Loss: 5.502, Test_Loss: 0.053\n",
      "Epoch 200: Train_Loss: 5.499, Test_Loss: 0.053\n",
      "Epoch 201: Train_Loss: 5.497, Test_Loss: 0.053\n",
      "Epoch 202: Train_Loss: 5.495, Test_Loss: 0.053\n",
      "Epoch 203: Train_Loss: 5.492, Test_Loss: 0.053\n",
      "Epoch 204: Train_Loss: 5.490, Test_Loss: 0.053\n",
      "Epoch 205: Train_Loss: 5.489, Test_Loss: 0.053\n",
      "Epoch 206: Train_Loss: 5.489, Test_Loss: 0.053\n",
      "Epoch 207: Train_Loss: 5.490, Test_Loss: 0.053\n",
      "Epoch 208: Train_Loss: 5.495, Test_Loss: 0.053\n",
      "Epoch 209: Train_Loss: 5.511, Test_Loss: 0.053\n",
      "Epoch 210: Train_Loss: 5.514, Test_Loss: 0.053\n",
      "Epoch 211: Train_Loss: 5.504, Test_Loss: 0.053\n",
      "Epoch 212: Train_Loss: 5.498, Test_Loss: 0.053\n",
      "Epoch 213: Train_Loss: 5.492, Test_Loss: 0.053\n",
      "Epoch 214: Train_Loss: 5.498, Test_Loss: 0.053\n",
      "Epoch 215: Train_Loss: 5.512, Test_Loss: 0.053\n",
      "Epoch 216: Train_Loss: 5.527, Test_Loss: 0.053\n",
      "Epoch 217: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 218: Train_Loss: 5.524, Test_Loss: 0.053\n",
      "Epoch 219: Train_Loss: 5.532, Test_Loss: 0.054\n",
      "Epoch 220: Train_Loss: 5.562, Test_Loss: 0.056\n",
      "Epoch 221: Train_Loss: 5.625, Test_Loss: 0.057\n",
      "Epoch 222: Train_Loss: 5.757, Test_Loss: 0.053\n",
      "Epoch 223: Train_Loss: 5.712, Test_Loss: 0.054\n",
      "Epoch 224: Train_Loss: 5.596, Test_Loss: 0.053\n",
      "Epoch 225: Train_Loss: 5.546, Test_Loss: 0.053\n",
      "Epoch 226: Train_Loss: 5.535, Test_Loss: 0.053\n",
      "Epoch 227: Train_Loss: 5.530, Test_Loss: 0.053\n",
      "Epoch 228: Train_Loss: 5.530, Test_Loss: 0.053\n",
      "Epoch 229: Train_Loss: 5.535, Test_Loss: 0.053\n",
      "Epoch 230: Train_Loss: 5.547, Test_Loss: 0.053\n",
      "Epoch 231: Train_Loss: 5.566, Test_Loss: 0.053\n",
      "Epoch 232: Train_Loss: 5.587, Test_Loss: 0.054\n",
      "Epoch 233: Train_Loss: 5.602, Test_Loss: 0.054\n",
      "Epoch 234: Train_Loss: 5.611, Test_Loss: 0.054\n",
      "Epoch 235: Train_Loss: 5.617, Test_Loss: 0.054\n",
      "Epoch 236: Train_Loss: 5.623, Test_Loss: 0.054\n",
      "Epoch 237: Train_Loss: 5.628, Test_Loss: 0.055\n",
      "Epoch 238: Train_Loss: 5.640, Test_Loss: 0.056\n",
      "Epoch 239: Train_Loss: 5.669, Test_Loss: 0.057\n",
      "Epoch 240: Train_Loss: 5.730, Test_Loss: 0.059\n",
      "Epoch 241: Train_Loss: 5.860, Test_Loss: 0.060\n",
      "Epoch 242: Train_Loss: 6.117, Test_Loss: 0.055\n",
      "Epoch 243: Train_Loss: 6.216, Test_Loss: 0.055\n",
      "Epoch 244: Train_Loss: 5.933, Test_Loss: 0.054\n",
      "Epoch 245: Train_Loss: 5.734, Test_Loss: 0.053\n",
      "Epoch 246: Train_Loss: 5.649, Test_Loss: 0.053\n",
      "Epoch 247: Train_Loss: 5.605, Test_Loss: 0.053\n",
      "Epoch 248: Train_Loss: 5.576, Test_Loss: 0.053\n",
      "Epoch 249: Train_Loss: 5.555, Test_Loss: 0.053\n",
      "Epoch 250: Train_Loss: 5.538, Test_Loss: 0.053\n",
      "Epoch 251: Train_Loss: 5.524, Test_Loss: 0.053\n",
      "Epoch 252: Train_Loss: 5.513, Test_Loss: 0.053\n",
      "Epoch 253: Train_Loss: 5.503, Test_Loss: 0.053\n",
      "Epoch 254: Train_Loss: 5.495, Test_Loss: 0.052\n",
      "Epoch 255: Train_Loss: 5.488, Test_Loss: 0.052\n",
      "Epoch 256: Train_Loss: 5.481, Test_Loss: 0.052\n",
      "Epoch 257: Train_Loss: 5.476, Test_Loss: 0.052\n",
      "Epoch 258: Train_Loss: 5.471, Test_Loss: 0.052\n",
      "Epoch 259: Train_Loss: 5.466, Test_Loss: 0.052\n",
      "Epoch 260: Train_Loss: 5.461, Test_Loss: 0.052\n",
      "Epoch 261: Train_Loss: 5.457, Test_Loss: 0.052\n",
      "Epoch 262: Train_Loss: 5.453, Test_Loss: 0.052\n",
      "Epoch 263: Train_Loss: 5.449, Test_Loss: 0.052\n",
      "Epoch 264: Train_Loss: 5.446, Test_Loss: 0.052\n",
      "Epoch 265: Train_Loss: 5.443, Test_Loss: 0.052\n",
      "Epoch 266: Train_Loss: 5.439, Test_Loss: 0.052\n",
      "Epoch 267: Train_Loss: 5.436, Test_Loss: 0.052\n",
      "Epoch 268: Train_Loss: 5.434, Test_Loss: 0.052\n",
      "Epoch 269: Train_Loss: 5.431, Test_Loss: 0.052\n",
      "Epoch 270: Train_Loss: 5.428, Test_Loss: 0.052\n",
      "Epoch 271: Train_Loss: 5.426, Test_Loss: 0.052\n",
      "Epoch 272: Train_Loss: 5.423, Test_Loss: 0.052\n",
      "Epoch 273: Train_Loss: 5.421, Test_Loss: 0.052\n",
      "Epoch 274: Train_Loss: 5.418, Test_Loss: 0.052\n",
      "Epoch 275: Train_Loss: 5.416, Test_Loss: 0.052\n",
      "Epoch 276: Train_Loss: 5.414, Test_Loss: 0.052\n",
      "Epoch 277: Train_Loss: 5.411, Test_Loss: 0.052\n",
      "Epoch 278: Train_Loss: 5.409, Test_Loss: 0.052\n",
      "Epoch 279: Train_Loss: 5.407, Test_Loss: 0.052\n",
      "Epoch 280: Train_Loss: 5.405, Test_Loss: 0.052\n",
      "Epoch 281: Train_Loss: 5.403, Test_Loss: 0.052\n",
      "Epoch 282: Train_Loss: 5.401, Test_Loss: 0.052\n",
      "Epoch 283: Train_Loss: 5.399, Test_Loss: 0.052\n",
      "Epoch 284: Train_Loss: 5.397, Test_Loss: 0.052\n",
      "Epoch 285: Train_Loss: 5.395, Test_Loss: 0.052\n",
      "Epoch 286: Train_Loss: 5.393, Test_Loss: 0.052\n",
      "Epoch 287: Train_Loss: 5.391, Test_Loss: 0.052\n",
      "Epoch 288: Train_Loss: 5.389, Test_Loss: 0.052\n",
      "Epoch 289: Train_Loss: 5.387, Test_Loss: 0.052\n",
      "Epoch 290: Train_Loss: 5.386, Test_Loss: 0.052\n",
      "Epoch 291: Train_Loss: 5.385, Test_Loss: 0.052\n",
      "Epoch 292: Train_Loss: 5.384, Test_Loss: 0.052\n",
      "Epoch 293: Train_Loss: 5.383, Test_Loss: 0.052\n",
      "Epoch 294: Train_Loss: 5.383, Test_Loss: 0.052\n",
      "Epoch 295: Train_Loss: 5.384, Test_Loss: 0.052\n",
      "Epoch 296: Train_Loss: 5.388, Test_Loss: 0.052\n",
      "Epoch 297: Train_Loss: 5.403, Test_Loss: 0.053\n",
      "Epoch 298: Train_Loss: 5.427, Test_Loss: 0.053\n",
      "Epoch 299: Train_Loss: 5.414, Test_Loss: 0.053\n",
      "Epoch 300: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 301: Train_Loss: 5.402, Test_Loss: 0.053\n",
      "Epoch 302: Train_Loss: 5.407, Test_Loss: 0.053\n",
      "Epoch 303: Train_Loss: 5.422, Test_Loss: 0.053\n",
      "Epoch 304: Train_Loss: 5.453, Test_Loss: 0.053\n",
      "Epoch 305: Train_Loss: 5.482, Test_Loss: 0.053\n",
      "Epoch 306: Train_Loss: 5.525, Test_Loss: 0.054\n",
      "Epoch 307: Train_Loss: 5.560, Test_Loss: 0.054\n",
      "Epoch 308: Train_Loss: 5.583, Test_Loss: 0.054\n",
      "Epoch 309: Train_Loss: 5.607, Test_Loss: 0.054\n",
      "Epoch 310: Train_Loss: 5.606, Test_Loss: 0.055\n",
      "Epoch 311: Train_Loss: 5.620, Test_Loss: 0.054\n",
      "Epoch 312: Train_Loss: 5.649, Test_Loss: 0.054\n",
      "Epoch 313: Train_Loss: 5.644, Test_Loss: 0.055\n",
      "Epoch 314: Train_Loss: 5.602, Test_Loss: 0.053\n",
      "Epoch 315: Train_Loss: 5.559, Test_Loss: 0.053\n",
      "Epoch 316: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 317: Train_Loss: 5.508, Test_Loss: 0.053\n",
      "Epoch 318: Train_Loss: 5.491, Test_Loss: 0.053\n",
      "Epoch 319: Train_Loss: 5.484, Test_Loss: 0.053\n",
      "Epoch 320: Train_Loss: 5.485, Test_Loss: 0.053\n",
      "Epoch 321: Train_Loss: 5.494, Test_Loss: 0.053\n",
      "Epoch 322: Train_Loss: 5.509, Test_Loss: 0.053\n",
      "Epoch 323: Train_Loss: 5.525, Test_Loss: 0.053\n",
      "Epoch 324: Train_Loss: 5.537, Test_Loss: 0.053\n",
      "Epoch 325: Train_Loss: 5.542, Test_Loss: 0.054\n",
      "Epoch 326: Train_Loss: 5.534, Test_Loss: 0.054\n",
      "Epoch 327: Train_Loss: 5.515, Test_Loss: 0.054\n",
      "Epoch 328: Train_Loss: 5.490, Test_Loss: 0.054\n",
      "Epoch 329: Train_Loss: 5.470, Test_Loss: 0.053\n",
      "Epoch 330: Train_Loss: 5.457, Test_Loss: 0.053\n",
      "Epoch 331: Train_Loss: 5.448, Test_Loss: 0.053\n",
      "Epoch 332: Train_Loss: 5.440, Test_Loss: 0.053\n",
      "Epoch 333: Train_Loss: 5.433, Test_Loss: 0.053\n",
      "Epoch 334: Train_Loss: 5.427, Test_Loss: 0.053\n",
      "Epoch 335: Train_Loss: 5.420, Test_Loss: 0.053\n",
      "Epoch 336: Train_Loss: 5.415, Test_Loss: 0.053\n",
      "Epoch 337: Train_Loss: 5.410, Test_Loss: 0.053\n",
      "Epoch 338: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 339: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 340: Train_Loss: 5.412, Test_Loss: 0.053\n",
      "Epoch 341: Train_Loss: 5.418, Test_Loss: 0.053\n",
      "Epoch 342: Train_Loss: 5.426, Test_Loss: 0.053\n",
      "Epoch 343: Train_Loss: 5.434, Test_Loss: 0.053\n",
      "Epoch 344: Train_Loss: 5.441, Test_Loss: 0.053\n",
      "Epoch 345: Train_Loss: 5.448, Test_Loss: 0.053\n",
      "Epoch 346: Train_Loss: 5.461, Test_Loss: 0.053\n",
      "Epoch 347: Train_Loss: 5.484, Test_Loss: 0.054\n",
      "Epoch 348: Train_Loss: 5.503, Test_Loss: 0.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349: Train_Loss: 5.499, Test_Loss: 0.055\n",
      "Epoch 350: Train_Loss: 5.487, Test_Loss: 0.054\n",
      "Epoch 351: Train_Loss: 5.487, Test_Loss: 0.054\n",
      "Epoch 352: Train_Loss: 5.497, Test_Loss: 0.054\n",
      "Epoch 353: Train_Loss: 5.510, Test_Loss: 0.054\n",
      "Epoch 354: Train_Loss: 5.529, Test_Loss: 0.054\n",
      "Epoch 355: Train_Loss: 5.566, Test_Loss: 0.055\n",
      "Epoch 356: Train_Loss: 5.645, Test_Loss: 0.057\n",
      "Epoch 357: Train_Loss: 5.807, Test_Loss: 0.061\n",
      "Epoch 358: Train_Loss: 6.134, Test_Loss: 0.061\n",
      "Epoch 359: Train_Loss: 6.290, Test_Loss: 0.054\n",
      "Epoch 360: Train_Loss: 5.914, Test_Loss: 0.053\n",
      "Epoch 361: Train_Loss: 5.710, Test_Loss: 0.053\n",
      "Epoch 362: Train_Loss: 5.607, Test_Loss: 0.053\n",
      "Epoch 363: Train_Loss: 5.545, Test_Loss: 0.053\n",
      "Epoch 364: Train_Loss: 5.505, Test_Loss: 0.053\n",
      "Epoch 365: Train_Loss: 5.476, Test_Loss: 0.053\n",
      "Epoch 366: Train_Loss: 5.454, Test_Loss: 0.052\n",
      "Epoch 367: Train_Loss: 5.438, Test_Loss: 0.052\n",
      "Epoch 368: Train_Loss: 5.424, Test_Loss: 0.052\n",
      "Epoch 369: Train_Loss: 5.413, Test_Loss: 0.052\n",
      "Epoch 370: Train_Loss: 5.403, Test_Loss: 0.052\n",
      "Epoch 371: Train_Loss: 5.394, Test_Loss: 0.052\n",
      "Epoch 372: Train_Loss: 5.385, Test_Loss: 0.052\n",
      "Epoch 373: Train_Loss: 5.378, Test_Loss: 0.052\n",
      "Epoch 374: Train_Loss: 5.371, Test_Loss: 0.052\n",
      "Epoch 375: Train_Loss: 5.364, Test_Loss: 0.052\n",
      "Epoch 376: Train_Loss: 5.358, Test_Loss: 0.052\n",
      "Epoch 377: Train_Loss: 5.353, Test_Loss: 0.052\n",
      "Epoch 378: Train_Loss: 5.348, Test_Loss: 0.052\n",
      "Epoch 379: Train_Loss: 5.343, Test_Loss: 0.052\n",
      "Epoch 380: Train_Loss: 5.338, Test_Loss: 0.052\n",
      "Epoch 381: Train_Loss: 5.334, Test_Loss: 0.052\n",
      "Epoch 382: Train_Loss: 5.330, Test_Loss: 0.052\n",
      "Epoch 383: Train_Loss: 5.326, Test_Loss: 0.052\n",
      "Epoch 384: Train_Loss: 5.323, Test_Loss: 0.052\n",
      "Epoch 385: Train_Loss: 5.320, Test_Loss: 0.052\n",
      "Epoch 386: Train_Loss: 5.316, Test_Loss: 0.052\n",
      "Epoch 387: Train_Loss: 5.313, Test_Loss: 0.052\n",
      "Epoch 388: Train_Loss: 5.310, Test_Loss: 0.052\n",
      "Epoch 389: Train_Loss: 5.307, Test_Loss: 0.052\n",
      "Epoch 390: Train_Loss: 5.304, Test_Loss: 0.052\n",
      "Epoch 391: Train_Loss: 5.302, Test_Loss: 0.052\n",
      "Epoch 392: Train_Loss: 5.299, Test_Loss: 0.052\n",
      "Epoch 393: Train_Loss: 5.296, Test_Loss: 0.052\n",
      "Epoch 394: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 395: Train_Loss: 5.291, Test_Loss: 0.052\n",
      "Epoch 396: Train_Loss: 5.289, Test_Loss: 0.052\n",
      "Epoch 397: Train_Loss: 5.287, Test_Loss: 0.052\n",
      "Epoch 398: Train_Loss: 5.284, Test_Loss: 0.052\n",
      "Epoch 399: Train_Loss: 5.282, Test_Loss: 0.052\n",
      "Epoch 400: Train_Loss: 5.281, Test_Loss: 0.052\n",
      "Epoch 401: Train_Loss: 5.279, Test_Loss: 0.052\n",
      "Epoch 402: Train_Loss: 5.277, Test_Loss: 0.052\n",
      "Epoch 403: Train_Loss: 5.276, Test_Loss: 0.052\n",
      "Epoch 404: Train_Loss: 5.275, Test_Loss: 0.052\n",
      "Epoch 405: Train_Loss: 5.274, Test_Loss: 0.052\n",
      "Epoch 406: Train_Loss: 5.275, Test_Loss: 0.052\n",
      "Epoch 407: Train_Loss: 5.281, Test_Loss: 0.052\n",
      "Epoch 408: Train_Loss: 5.292, Test_Loss: 0.052\n",
      "Epoch 409: Train_Loss: 5.305, Test_Loss: 0.052\n",
      "Epoch 410: Train_Loss: 5.296, Test_Loss: 0.052\n",
      "Epoch 411: Train_Loss: 5.291, Test_Loss: 0.052\n",
      "Epoch 412: Train_Loss: 5.301, Test_Loss: 0.052\n",
      "Epoch 413: Train_Loss: 5.314, Test_Loss: 0.052\n",
      "Epoch 414: Train_Loss: 5.316, Test_Loss: 0.052\n",
      "Epoch 415: Train_Loss: 5.310, Test_Loss: 0.052\n",
      "Epoch 416: Train_Loss: 5.304, Test_Loss: 0.052\n",
      "Epoch 417: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 418: Train_Loss: 5.295, Test_Loss: 0.052\n",
      "Epoch 419: Train_Loss: 5.300, Test_Loss: 0.052\n",
      "Epoch 420: Train_Loss: 5.315, Test_Loss: 0.052\n",
      "Epoch 421: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 422: Train_Loss: 5.356, Test_Loss: 0.053\n",
      "Epoch 423: Train_Loss: 5.388, Test_Loss: 0.053\n",
      "Epoch 424: Train_Loss: 5.424, Test_Loss: 0.054\n",
      "Epoch 425: Train_Loss: 5.442, Test_Loss: 0.056\n",
      "Epoch 426: Train_Loss: 5.476, Test_Loss: 0.054\n",
      "Epoch 427: Train_Loss: 5.480, Test_Loss: 0.053\n",
      "Epoch 428: Train_Loss: 5.406, Test_Loss: 0.053\n",
      "Epoch 429: Train_Loss: 5.353, Test_Loss: 0.052\n",
      "Epoch 430: Train_Loss: 5.344, Test_Loss: 0.052\n",
      "Epoch 431: Train_Loss: 5.366, Test_Loss: 0.053\n",
      "Epoch 432: Train_Loss: 5.378, Test_Loss: 0.053\n",
      "Epoch 433: Train_Loss: 5.368, Test_Loss: 0.053\n",
      "Epoch 434: Train_Loss: 5.361, Test_Loss: 0.052\n",
      "Epoch 435: Train_Loss: 5.363, Test_Loss: 0.053\n",
      "Epoch 436: Train_Loss: 5.368, Test_Loss: 0.053\n",
      "Epoch 437: Train_Loss: 5.360, Test_Loss: 0.053\n",
      "Epoch 438: Train_Loss: 5.346, Test_Loss: 0.053\n",
      "Epoch 439: Train_Loss: 5.342, Test_Loss: 0.053\n",
      "Epoch 440: Train_Loss: 5.340, Test_Loss: 0.053\n",
      "Epoch 441: Train_Loss: 5.333, Test_Loss: 0.053\n",
      "Epoch 442: Train_Loss: 5.329, Test_Loss: 0.053\n",
      "Epoch 443: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 444: Train_Loss: 5.352, Test_Loss: 0.052\n",
      "Epoch 445: Train_Loss: 5.363, Test_Loss: 0.052\n",
      "Epoch 446: Train_Loss: 5.365, Test_Loss: 0.052\n",
      "Epoch 447: Train_Loss: 5.352, Test_Loss: 0.052\n",
      "Epoch 448: Train_Loss: 5.334, Test_Loss: 0.052\n",
      "Epoch 449: Train_Loss: 5.323, Test_Loss: 0.052\n",
      "Epoch 450: Train_Loss: 5.321, Test_Loss: 0.052\n",
      "Epoch 451: Train_Loss: 5.326, Test_Loss: 0.053\n",
      "Epoch 452: Train_Loss: 5.328, Test_Loss: 0.053\n",
      "Epoch 453: Train_Loss: 5.324, Test_Loss: 0.053\n",
      "Epoch 454: Train_Loss: 5.316, Test_Loss: 0.053\n",
      "Epoch 455: Train_Loss: 5.313, Test_Loss: 0.052\n",
      "Epoch 456: Train_Loss: 5.319, Test_Loss: 0.052\n",
      "Epoch 457: Train_Loss: 5.328, Test_Loss: 0.052\n",
      "Epoch 458: Train_Loss: 5.333, Test_Loss: 0.052\n",
      "Epoch 459: Train_Loss: 5.332, Test_Loss: 0.052\n",
      "Epoch 460: Train_Loss: 5.332, Test_Loss: 0.052\n",
      "Epoch 461: Train_Loss: 5.338, Test_Loss: 0.053\n",
      "Epoch 462: Train_Loss: 5.344, Test_Loss: 0.053\n",
      "Epoch 463: Train_Loss: 5.349, Test_Loss: 0.054\n",
      "Epoch 464: Train_Loss: 5.359, Test_Loss: 0.054\n",
      "Epoch 465: Train_Loss: 5.372, Test_Loss: 0.055\n",
      "Epoch 466: Train_Loss: 5.388, Test_Loss: 0.056\n",
      "Epoch 467: Train_Loss: 5.418, Test_Loss: 0.057\n",
      "Epoch 468: Train_Loss: 5.473, Test_Loss: 0.059\n",
      "Epoch 469: Train_Loss: 5.583, Test_Loss: 0.062\n",
      "Epoch 470: Train_Loss: 5.772, Test_Loss: 0.064\n",
      "Epoch 471: Train_Loss: 6.014, Test_Loss: 0.056\n",
      "Epoch 472: Train_Loss: 5.982, Test_Loss: 0.054\n",
      "Epoch 473: Train_Loss: 5.759, Test_Loss: 0.053\n",
      "Epoch 474: Train_Loss: 5.575, Test_Loss: 0.053\n",
      "Epoch 475: Train_Loss: 5.464, Test_Loss: 0.052\n",
      "Epoch 476: Train_Loss: 5.402, Test_Loss: 0.052\n",
      "Epoch 477: Train_Loss: 5.363, Test_Loss: 0.052\n",
      "Epoch 478: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 479: Train_Loss: 5.312, Test_Loss: 0.052\n",
      "Epoch 480: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 481: Train_Loss: 5.280, Test_Loss: 0.052\n",
      "Epoch 482: Train_Loss: 5.268, Test_Loss: 0.052\n",
      "Epoch 483: Train_Loss: 5.259, Test_Loss: 0.052\n",
      "Epoch 484: Train_Loss: 5.251, Test_Loss: 0.052\n",
      "Epoch 485: Train_Loss: 5.244, Test_Loss: 0.052\n",
      "Epoch 486: Train_Loss: 5.238, Test_Loss: 0.052\n",
      "Epoch 487: Train_Loss: 5.233, Test_Loss: 0.052\n",
      "Epoch 488: Train_Loss: 5.229, Test_Loss: 0.052\n",
      "Epoch 489: Train_Loss: 5.225, Test_Loss: 0.052\n",
      "Epoch 490: Train_Loss: 5.221, Test_Loss: 0.052\n",
      "Epoch 491: Train_Loss: 5.217, Test_Loss: 0.052\n",
      "Epoch 492: Train_Loss: 5.214, Test_Loss: 0.052\n",
      "Epoch 493: Train_Loss: 5.211, Test_Loss: 0.052\n",
      "Epoch 494: Train_Loss: 5.208, Test_Loss: 0.052\n",
      "Epoch 495: Train_Loss: 5.205, Test_Loss: 0.052\n",
      "Epoch 496: Train_Loss: 5.203, Test_Loss: 0.052\n",
      "Epoch 497: Train_Loss: 5.201, Test_Loss: 0.052\n",
      "Epoch 498: Train_Loss: 5.198, Test_Loss: 0.052\n",
      "Epoch 499: Train_Loss: 5.197, Test_Loss: 0.052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 500\n",
    "net.load_weights('easy_checkpoint')\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step(net, example, opt)\n",
    "        total_loss += loss\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    \n",
    "    test_loss = test(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss/len(data), test_loss))\n",
    "    \n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('easy_checkpoint')\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_weights('easy_checkpoint')\n",
    "em = tf.constant(test_data[10].reshape(1,64,64,1))\n",
    "ps = net(tf.cast(em,dtype=tf.float32))\n",
    "\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[10].reshape(64,64)\n",
    "ps[ps>1]=1\n",
    "ps[ps<=0.01]=0\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3dcc3ea650>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3dcc31ca50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net.save_weights('stage1.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (64, 1) and (256, 64) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c252de03adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stage1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1221\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    697\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    698\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3321\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3322\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    821\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \"\"\"\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (64, 1) and (256, 64) are incompatible"
     ]
    }
   ],
   "source": [
    "net.load_weights('stage1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
