{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from bspnet2D import BspNet2D\n",
    "from loss_func import stage1_loss, rec_loss\n",
    "batchsize = 32\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['pixels']>\n",
      "Keys: <KeysViewHDF5 ['pixels']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('complex_elements.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    \n",
    "    \n",
    "with h5py.File('complex_elements_test.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    test_data = list(f[a_group_key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2000\n",
      "Testing Dataset Length: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Length: %d\" % len(data))\n",
    "print(\"Testing Dataset Length: %d\" % len(test_data))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batchsize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BspNet2D(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage1_loss(output, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def test(net, test_ds):\n",
    "    total_loss = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        S = net(example)\n",
    "        loss = stage1_loss(S, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "        total_loss += loss\n",
    "    return total_loss/len(test_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.05454742, shape=(), dtype=float32)\n",
      "Epoch 000: Train_Loss: 5.927, Test_Loss: 0.055\n",
      "Epoch 001: Train_Loss: 5.923, Test_Loss: 0.055\n",
      "Epoch 002: Train_Loss: 5.919, Test_Loss: 0.054\n",
      "Epoch 003: Train_Loss: 5.915, Test_Loss: 0.054\n",
      "Epoch 004: Train_Loss: 5.911, Test_Loss: 0.054\n",
      "Epoch 005: Train_Loss: 5.907, Test_Loss: 0.054\n",
      "Epoch 006: Train_Loss: 5.904, Test_Loss: 0.054\n",
      "Epoch 007: Train_Loss: 5.900, Test_Loss: 0.054\n",
      "Epoch 008: Train_Loss: 5.896, Test_Loss: 0.054\n",
      "Epoch 009: Train_Loss: 5.892, Test_Loss: 0.054\n",
      "Epoch 010: Train_Loss: 5.889, Test_Loss: 0.054\n",
      "Epoch 011: Train_Loss: 5.885, Test_Loss: 0.054\n",
      "Epoch 012: Train_Loss: 5.881, Test_Loss: 0.054\n",
      "Epoch 013: Train_Loss: 5.877, Test_Loss: 0.054\n",
      "Epoch 014: Train_Loss: 5.874, Test_Loss: 0.054\n",
      "Epoch 015: Train_Loss: 5.870, Test_Loss: 0.054\n",
      "Epoch 016: Train_Loss: 5.866, Test_Loss: 0.054\n",
      "Epoch 017: Train_Loss: 5.862, Test_Loss: 0.054\n",
      "Epoch 018: Train_Loss: 5.859, Test_Loss: 0.054\n",
      "Epoch 019: Train_Loss: 5.855, Test_Loss: 0.054\n",
      "Epoch 020: Train_Loss: 5.851, Test_Loss: 0.054\n",
      "Epoch 021: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 022: Train_Loss: 5.843, Test_Loss: 0.054\n",
      "Epoch 023: Train_Loss: 5.839, Test_Loss: 0.054\n",
      "Epoch 024: Train_Loss: 5.836, Test_Loss: 0.054\n",
      "Epoch 025: Train_Loss: 5.832, Test_Loss: 0.054\n",
      "Epoch 026: Train_Loss: 5.828, Test_Loss: 0.054\n",
      "Epoch 027: Train_Loss: 5.824, Test_Loss: 0.054\n",
      "Epoch 028: Train_Loss: 5.821, Test_Loss: 0.054\n",
      "Epoch 029: Train_Loss: 5.817, Test_Loss: 0.054\n",
      "Epoch 030: Train_Loss: 5.814, Test_Loss: 0.054\n",
      "Epoch 031: Train_Loss: 5.810, Test_Loss: 0.054\n",
      "Epoch 032: Train_Loss: 5.807, Test_Loss: 0.054\n",
      "Epoch 033: Train_Loss: 5.804, Test_Loss: 0.054\n",
      "Epoch 034: Train_Loss: 5.802, Test_Loss: 0.054\n",
      "Epoch 035: Train_Loss: 5.800, Test_Loss: 0.054\n",
      "Epoch 036: Train_Loss: 5.799, Test_Loss: 0.054\n",
      "Epoch 037: Train_Loss: 5.799, Test_Loss: 0.054\n",
      "Epoch 038: Train_Loss: 5.803, Test_Loss: 0.054\n",
      "Epoch 039: Train_Loss: 5.815, Test_Loss: 0.055\n",
      "Epoch 040: Train_Loss: 5.844, Test_Loss: 0.055\n",
      "Epoch 041: Train_Loss: 5.901, Test_Loss: 0.056\n",
      "Epoch 042: Train_Loss: 5.973, Test_Loss: 0.056\n",
      "Epoch 043: Train_Loss: 5.966, Test_Loss: 0.058\n",
      "Epoch 044: Train_Loss: 5.982, Test_Loss: 0.057\n",
      "Epoch 045: Train_Loss: 6.055, Test_Loss: 0.058\n",
      "Epoch 046: Train_Loss: 6.016, Test_Loss: 0.055\n",
      "Epoch 047: Train_Loss: 5.888, Test_Loss: 0.055\n",
      "Epoch 048: Train_Loss: 5.857, Test_Loss: 0.054\n",
      "Epoch 049: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 050: Train_Loss: 5.844, Test_Loss: 0.054\n",
      "Epoch 051: Train_Loss: 5.842, Test_Loss: 0.054\n",
      "Epoch 052: Train_Loss: 5.841, Test_Loss: 0.054\n",
      "Epoch 053: Train_Loss: 5.842, Test_Loss: 0.054\n",
      "Epoch 054: Train_Loss: 5.847, Test_Loss: 0.054\n",
      "Epoch 055: Train_Loss: 5.855, Test_Loss: 0.054\n",
      "Epoch 056: Train_Loss: 5.867, Test_Loss: 0.055\n",
      "Epoch 057: Train_Loss: 5.888, Test_Loss: 0.055\n",
      "Epoch 058: Train_Loss: 5.928, Test_Loss: 0.057\n",
      "Epoch 059: Train_Loss: 6.009, Test_Loss: 0.059\n",
      "Epoch 060: Train_Loss: 6.184, Test_Loss: 0.058\n",
      "Epoch 061: Train_Loss: 6.383, Test_Loss: 0.055\n",
      "Epoch 062: Train_Loss: 6.198, Test_Loss: 0.056\n",
      "Epoch 063: Train_Loss: 5.997, Test_Loss: 0.055\n",
      "Epoch 064: Train_Loss: 5.912, Test_Loss: 0.055\n",
      "Epoch 065: Train_Loss: 5.871, Test_Loss: 0.055\n",
      "Epoch 066: Train_Loss: 5.846, Test_Loss: 0.054\n",
      "Epoch 067: Train_Loss: 5.828, Test_Loss: 0.054\n",
      "Epoch 068: Train_Loss: 5.813, Test_Loss: 0.054\n",
      "Epoch 069: Train_Loss: 5.801, Test_Loss: 0.054\n",
      "Epoch 070: Train_Loss: 5.791, Test_Loss: 0.054\n",
      "Epoch 071: Train_Loss: 5.783, Test_Loss: 0.054\n",
      "Epoch 072: Train_Loss: 5.776, Test_Loss: 0.054\n",
      "Epoch 073: Train_Loss: 5.770, Test_Loss: 0.054\n",
      "Epoch 074: Train_Loss: 5.764, Test_Loss: 0.054\n",
      "Epoch 075: Train_Loss: 5.759, Test_Loss: 0.054\n",
      "Epoch 076: Train_Loss: 5.754, Test_Loss: 0.054\n",
      "Epoch 077: Train_Loss: 5.750, Test_Loss: 0.054\n",
      "Epoch 078: Train_Loss: 5.746, Test_Loss: 0.054\n",
      "Epoch 079: Train_Loss: 5.742, Test_Loss: 0.054\n",
      "Epoch 080: Train_Loss: 5.738, Test_Loss: 0.054\n",
      "Epoch 081: Train_Loss: 5.734, Test_Loss: 0.054\n",
      "Epoch 082: Train_Loss: 5.731, Test_Loss: 0.054\n",
      "Epoch 083: Train_Loss: 5.727, Test_Loss: 0.054\n",
      "Epoch 084: Train_Loss: 5.724, Test_Loss: 0.054\n",
      "Epoch 085: Train_Loss: 5.720, Test_Loss: 0.054\n",
      "Epoch 086: Train_Loss: 5.717, Test_Loss: 0.054\n",
      "Epoch 087: Train_Loss: 5.714, Test_Loss: 0.054\n",
      "Epoch 088: Train_Loss: 5.710, Test_Loss: 0.054\n",
      "Epoch 089: Train_Loss: 5.707, Test_Loss: 0.054\n",
      "Epoch 090: Train_Loss: 5.704, Test_Loss: 0.054\n",
      "Epoch 091: Train_Loss: 5.701, Test_Loss: 0.054\n",
      "Epoch 092: Train_Loss: 5.698, Test_Loss: 0.054\n",
      "Epoch 093: Train_Loss: 5.694, Test_Loss: 0.053\n",
      "Epoch 094: Train_Loss: 5.691, Test_Loss: 0.053\n",
      "Epoch 095: Train_Loss: 5.688, Test_Loss: 0.053\n",
      "Epoch 096: Train_Loss: 5.685, Test_Loss: 0.053\n",
      "Epoch 097: Train_Loss: 5.682, Test_Loss: 0.053\n",
      "Epoch 098: Train_Loss: 5.679, Test_Loss: 0.053\n",
      "Epoch 099: Train_Loss: 5.676, Test_Loss: 0.053\n",
      "Epoch 100: Train_Loss: 5.673, Test_Loss: 0.053\n",
      "Epoch 101: Train_Loss: 5.670, Test_Loss: 0.053\n",
      "Epoch 102: Train_Loss: 5.667, Test_Loss: 0.053\n",
      "Epoch 103: Train_Loss: 5.663, Test_Loss: 0.053\n",
      "Epoch 104: Train_Loss: 5.660, Test_Loss: 0.053\n",
      "Epoch 105: Train_Loss: 5.657, Test_Loss: 0.053\n",
      "Epoch 106: Train_Loss: 5.654, Test_Loss: 0.053\n",
      "Epoch 107: Train_Loss: 5.651, Test_Loss: 0.053\n",
      "Epoch 108: Train_Loss: 5.648, Test_Loss: 0.053\n",
      "Epoch 109: Train_Loss: 5.646, Test_Loss: 0.053\n",
      "Epoch 110: Train_Loss: 5.643, Test_Loss: 0.053\n",
      "Epoch 111: Train_Loss: 5.640, Test_Loss: 0.053\n",
      "Epoch 112: Train_Loss: 5.637, Test_Loss: 0.053\n",
      "Epoch 113: Train_Loss: 5.635, Test_Loss: 0.053\n",
      "Epoch 114: Train_Loss: 5.632, Test_Loss: 0.053\n",
      "Epoch 115: Train_Loss: 5.630, Test_Loss: 0.053\n",
      "Epoch 116: Train_Loss: 5.629, Test_Loss: 0.053\n",
      "Epoch 117: Train_Loss: 5.630, Test_Loss: 0.053\n",
      "Epoch 118: Train_Loss: 5.634, Test_Loss: 0.053\n",
      "Epoch 119: Train_Loss: 5.642, Test_Loss: 0.053\n",
      "Epoch 120: Train_Loss: 5.655, Test_Loss: 0.054\n",
      "Epoch 121: Train_Loss: 5.683, Test_Loss: 0.054\n",
      "Epoch 122: Train_Loss: 5.698, Test_Loss: 0.054\n",
      "Epoch 123: Train_Loss: 5.679, Test_Loss: 0.054\n",
      "Epoch 124: Train_Loss: 5.663, Test_Loss: 0.054\n",
      "Epoch 125: Train_Loss: 5.656, Test_Loss: 0.054\n",
      "Epoch 126: Train_Loss: 5.668, Test_Loss: 0.054\n",
      "Epoch 127: Train_Loss: 5.693, Test_Loss: 0.054\n",
      "Epoch 128: Train_Loss: 5.754, Test_Loss: 0.055\n",
      "Epoch 129: Train_Loss: 5.869, Test_Loss: 0.060\n",
      "Epoch 130: Train_Loss: 5.880, Test_Loss: 0.054\n",
      "Epoch 131: Train_Loss: 5.697, Test_Loss: 0.054\n",
      "Epoch 132: Train_Loss: 5.669, Test_Loss: 0.054\n",
      "Epoch 133: Train_Loss: 5.664, Test_Loss: 0.054\n",
      "Epoch 134: Train_Loss: 5.663, Test_Loss: 0.054\n",
      "Epoch 135: Train_Loss: 5.664, Test_Loss: 0.054\n",
      "Epoch 136: Train_Loss: 5.667, Test_Loss: 0.054\n",
      "Epoch 137: Train_Loss: 5.673, Test_Loss: 0.054\n",
      "Epoch 138: Train_Loss: 5.679, Test_Loss: 0.054\n",
      "Epoch 139: Train_Loss: 5.685, Test_Loss: 0.054\n",
      "Epoch 140: Train_Loss: 5.688, Test_Loss: 0.054\n",
      "Epoch 141: Train_Loss: 5.689, Test_Loss: 0.054\n",
      "Epoch 142: Train_Loss: 5.691, Test_Loss: 0.054\n",
      "Epoch 143: Train_Loss: 5.693, Test_Loss: 0.055\n",
      "Epoch 144: Train_Loss: 5.697, Test_Loss: 0.055\n",
      "Epoch 145: Train_Loss: 5.700, Test_Loss: 0.055\n",
      "Epoch 146: Train_Loss: 5.706, Test_Loss: 0.055\n",
      "Epoch 147: Train_Loss: 5.717, Test_Loss: 0.055\n",
      "Epoch 148: Train_Loss: 5.735, Test_Loss: 0.055\n",
      "Epoch 149: Train_Loss: 5.763, Test_Loss: 0.055\n",
      "Epoch 150: Train_Loss: 5.805, Test_Loss: 0.055\n",
      "Epoch 151: Train_Loss: 5.876, Test_Loss: 0.056\n",
      "Epoch 152: Train_Loss: 6.023, Test_Loss: 0.058\n",
      "Epoch 153: Train_Loss: 6.331, Test_Loss: 0.061\n",
      "Epoch 154: Train_Loss: 6.374, Test_Loss: 0.055\n",
      "Epoch 155: Train_Loss: 6.032, Test_Loss: 0.054\n",
      "Epoch 156: Train_Loss: 5.885, Test_Loss: 0.054\n",
      "Epoch 157: Train_Loss: 5.810, Test_Loss: 0.054\n",
      "Epoch 158: Train_Loss: 5.762, Test_Loss: 0.053\n",
      "Epoch 159: Train_Loss: 5.728, Test_Loss: 0.053\n",
      "Epoch 160: Train_Loss: 5.702, Test_Loss: 0.053\n",
      "Epoch 161: Train_Loss: 5.682, Test_Loss: 0.053\n",
      "Epoch 162: Train_Loss: 5.667, Test_Loss: 0.053\n",
      "Epoch 163: Train_Loss: 5.654, Test_Loss: 0.053\n",
      "Epoch 164: Train_Loss: 5.643, Test_Loss: 0.053\n",
      "Epoch 165: Train_Loss: 5.633, Test_Loss: 0.053\n",
      "Epoch 166: Train_Loss: 5.625, Test_Loss: 0.053\n",
      "Epoch 167: Train_Loss: 5.617, Test_Loss: 0.053\n",
      "Epoch 168: Train_Loss: 5.611, Test_Loss: 0.053\n",
      "Epoch 169: Train_Loss: 5.604, Test_Loss: 0.053\n",
      "Epoch 170: Train_Loss: 5.599, Test_Loss: 0.053\n",
      "Epoch 171: Train_Loss: 5.593, Test_Loss: 0.053\n",
      "Epoch 172: Train_Loss: 5.588, Test_Loss: 0.053\n",
      "Epoch 173: Train_Loss: 5.584, Test_Loss: 0.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174: Train_Loss: 5.579, Test_Loss: 0.053\n",
      "Epoch 175: Train_Loss: 5.575, Test_Loss: 0.053\n",
      "Epoch 176: Train_Loss: 5.571, Test_Loss: 0.053\n",
      "Epoch 177: Train_Loss: 5.567, Test_Loss: 0.053\n",
      "Epoch 178: Train_Loss: 5.564, Test_Loss: 0.053\n",
      "Epoch 179: Train_Loss: 5.560, Test_Loss: 0.053\n",
      "Epoch 180: Train_Loss: 5.556, Test_Loss: 0.053\n",
      "Epoch 181: Train_Loss: 5.553, Test_Loss: 0.053\n",
      "Epoch 182: Train_Loss: 5.550, Test_Loss: 0.053\n",
      "Epoch 183: Train_Loss: 5.547, Test_Loss: 0.053\n",
      "Epoch 184: Train_Loss: 5.544, Test_Loss: 0.053\n",
      "Epoch 185: Train_Loss: 5.540, Test_Loss: 0.053\n",
      "Epoch 186: Train_Loss: 5.537, Test_Loss: 0.053\n",
      "Epoch 187: Train_Loss: 5.534, Test_Loss: 0.053\n",
      "Epoch 188: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 189: Train_Loss: 5.529, Test_Loss: 0.053\n",
      "Epoch 190: Train_Loss: 5.526, Test_Loss: 0.053\n",
      "Epoch 191: Train_Loss: 5.523, Test_Loss: 0.053\n",
      "Epoch 192: Train_Loss: 5.520, Test_Loss: 0.053\n",
      "Epoch 193: Train_Loss: 5.518, Test_Loss: 0.053\n",
      "Epoch 194: Train_Loss: 5.515, Test_Loss: 0.053\n",
      "Epoch 195: Train_Loss: 5.512, Test_Loss: 0.053\n",
      "Epoch 196: Train_Loss: 5.510, Test_Loss: 0.053\n",
      "Epoch 197: Train_Loss: 5.507, Test_Loss: 0.053\n",
      "Epoch 198: Train_Loss: 5.504, Test_Loss: 0.053\n",
      "Epoch 199: Train_Loss: 5.502, Test_Loss: 0.053\n",
      "Epoch 200: Train_Loss: 5.499, Test_Loss: 0.053\n",
      "Epoch 201: Train_Loss: 5.497, Test_Loss: 0.053\n",
      "Epoch 202: Train_Loss: 5.495, Test_Loss: 0.053\n",
      "Epoch 203: Train_Loss: 5.492, Test_Loss: 0.053\n",
      "Epoch 204: Train_Loss: 5.490, Test_Loss: 0.053\n",
      "Epoch 205: Train_Loss: 5.489, Test_Loss: 0.053\n",
      "Epoch 206: Train_Loss: 5.489, Test_Loss: 0.053\n",
      "Epoch 207: Train_Loss: 5.490, Test_Loss: 0.053\n",
      "Epoch 208: Train_Loss: 5.495, Test_Loss: 0.053\n",
      "Epoch 209: Train_Loss: 5.511, Test_Loss: 0.053\n",
      "Epoch 210: Train_Loss: 5.514, Test_Loss: 0.053\n",
      "Epoch 211: Train_Loss: 5.504, Test_Loss: 0.053\n",
      "Epoch 212: Train_Loss: 5.498, Test_Loss: 0.053\n",
      "Epoch 213: Train_Loss: 5.492, Test_Loss: 0.053\n",
      "Epoch 214: Train_Loss: 5.498, Test_Loss: 0.053\n",
      "Epoch 215: Train_Loss: 5.512, Test_Loss: 0.053\n",
      "Epoch 216: Train_Loss: 5.527, Test_Loss: 0.053\n",
      "Epoch 217: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 218: Train_Loss: 5.524, Test_Loss: 0.053\n",
      "Epoch 219: Train_Loss: 5.532, Test_Loss: 0.054\n",
      "Epoch 220: Train_Loss: 5.562, Test_Loss: 0.056\n",
      "Epoch 221: Train_Loss: 5.625, Test_Loss: 0.057\n",
      "Epoch 222: Train_Loss: 5.757, Test_Loss: 0.053\n",
      "Epoch 223: Train_Loss: 5.712, Test_Loss: 0.054\n",
      "Epoch 224: Train_Loss: 5.596, Test_Loss: 0.053\n",
      "Epoch 225: Train_Loss: 5.546, Test_Loss: 0.053\n",
      "Epoch 226: Train_Loss: 5.535, Test_Loss: 0.053\n",
      "Epoch 227: Train_Loss: 5.530, Test_Loss: 0.053\n",
      "Epoch 228: Train_Loss: 5.530, Test_Loss: 0.053\n",
      "Epoch 229: Train_Loss: 5.535, Test_Loss: 0.053\n",
      "Epoch 230: Train_Loss: 5.547, Test_Loss: 0.053\n",
      "Epoch 231: Train_Loss: 5.566, Test_Loss: 0.053\n",
      "Epoch 232: Train_Loss: 5.587, Test_Loss: 0.054\n",
      "Epoch 233: Train_Loss: 5.602, Test_Loss: 0.054\n",
      "Epoch 234: Train_Loss: 5.611, Test_Loss: 0.054\n",
      "Epoch 235: Train_Loss: 5.617, Test_Loss: 0.054\n",
      "Epoch 236: Train_Loss: 5.623, Test_Loss: 0.054\n",
      "Epoch 237: Train_Loss: 5.628, Test_Loss: 0.055\n",
      "Epoch 238: Train_Loss: 5.640, Test_Loss: 0.056\n",
      "Epoch 239: Train_Loss: 5.669, Test_Loss: 0.057\n",
      "Epoch 240: Train_Loss: 5.730, Test_Loss: 0.059\n",
      "Epoch 241: Train_Loss: 5.860, Test_Loss: 0.060\n",
      "Epoch 242: Train_Loss: 6.117, Test_Loss: 0.055\n",
      "Epoch 243: Train_Loss: 6.216, Test_Loss: 0.055\n",
      "Epoch 244: Train_Loss: 5.933, Test_Loss: 0.054\n",
      "Epoch 245: Train_Loss: 5.734, Test_Loss: 0.053\n",
      "Epoch 246: Train_Loss: 5.649, Test_Loss: 0.053\n",
      "Epoch 247: Train_Loss: 5.605, Test_Loss: 0.053\n",
      "Epoch 248: Train_Loss: 5.576, Test_Loss: 0.053\n",
      "Epoch 249: Train_Loss: 5.555, Test_Loss: 0.053\n",
      "Epoch 250: Train_Loss: 5.538, Test_Loss: 0.053\n",
      "Epoch 251: Train_Loss: 5.524, Test_Loss: 0.053\n",
      "Epoch 252: Train_Loss: 5.513, Test_Loss: 0.053\n",
      "Epoch 253: Train_Loss: 5.503, Test_Loss: 0.053\n",
      "Epoch 254: Train_Loss: 5.495, Test_Loss: 0.052\n",
      "Epoch 255: Train_Loss: 5.488, Test_Loss: 0.052\n",
      "Epoch 256: Train_Loss: 5.481, Test_Loss: 0.052\n",
      "Epoch 257: Train_Loss: 5.476, Test_Loss: 0.052\n",
      "Epoch 258: Train_Loss: 5.471, Test_Loss: 0.052\n",
      "Epoch 259: Train_Loss: 5.466, Test_Loss: 0.052\n",
      "Epoch 260: Train_Loss: 5.461, Test_Loss: 0.052\n",
      "Epoch 261: Train_Loss: 5.457, Test_Loss: 0.052\n",
      "Epoch 262: Train_Loss: 5.453, Test_Loss: 0.052\n",
      "Epoch 263: Train_Loss: 5.449, Test_Loss: 0.052\n",
      "Epoch 264: Train_Loss: 5.446, Test_Loss: 0.052\n",
      "Epoch 265: Train_Loss: 5.443, Test_Loss: 0.052\n",
      "Epoch 266: Train_Loss: 5.439, Test_Loss: 0.052\n",
      "Epoch 267: Train_Loss: 5.436, Test_Loss: 0.052\n",
      "Epoch 268: Train_Loss: 5.434, Test_Loss: 0.052\n",
      "Epoch 269: Train_Loss: 5.431, Test_Loss: 0.052\n",
      "Epoch 270: Train_Loss: 5.428, Test_Loss: 0.052\n",
      "Epoch 271: Train_Loss: 5.426, Test_Loss: 0.052\n",
      "Epoch 272: Train_Loss: 5.423, Test_Loss: 0.052\n",
      "Epoch 273: Train_Loss: 5.421, Test_Loss: 0.052\n",
      "Epoch 274: Train_Loss: 5.418, Test_Loss: 0.052\n",
      "Epoch 275: Train_Loss: 5.416, Test_Loss: 0.052\n",
      "Epoch 276: Train_Loss: 5.414, Test_Loss: 0.052\n",
      "Epoch 277: Train_Loss: 5.411, Test_Loss: 0.052\n",
      "Epoch 278: Train_Loss: 5.409, Test_Loss: 0.052\n",
      "Epoch 279: Train_Loss: 5.407, Test_Loss: 0.052\n",
      "Epoch 280: Train_Loss: 5.405, Test_Loss: 0.052\n",
      "Epoch 281: Train_Loss: 5.403, Test_Loss: 0.052\n",
      "Epoch 282: Train_Loss: 5.401, Test_Loss: 0.052\n",
      "Epoch 283: Train_Loss: 5.399, Test_Loss: 0.052\n",
      "Epoch 284: Train_Loss: 5.397, Test_Loss: 0.052\n",
      "Epoch 285: Train_Loss: 5.395, Test_Loss: 0.052\n",
      "Epoch 286: Train_Loss: 5.393, Test_Loss: 0.052\n",
      "Epoch 287: Train_Loss: 5.391, Test_Loss: 0.052\n",
      "Epoch 288: Train_Loss: 5.389, Test_Loss: 0.052\n",
      "Epoch 289: Train_Loss: 5.387, Test_Loss: 0.052\n",
      "Epoch 290: Train_Loss: 5.386, Test_Loss: 0.052\n",
      "Epoch 291: Train_Loss: 5.385, Test_Loss: 0.052\n",
      "Epoch 292: Train_Loss: 5.384, Test_Loss: 0.052\n",
      "Epoch 293: Train_Loss: 5.383, Test_Loss: 0.052\n",
      "Epoch 294: Train_Loss: 5.383, Test_Loss: 0.052\n",
      "Epoch 295: Train_Loss: 5.384, Test_Loss: 0.052\n",
      "Epoch 296: Train_Loss: 5.388, Test_Loss: 0.052\n",
      "Epoch 297: Train_Loss: 5.403, Test_Loss: 0.053\n",
      "Epoch 298: Train_Loss: 5.427, Test_Loss: 0.053\n",
      "Epoch 299: Train_Loss: 5.414, Test_Loss: 0.053\n",
      "Epoch 300: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 301: Train_Loss: 5.402, Test_Loss: 0.053\n",
      "Epoch 302: Train_Loss: 5.407, Test_Loss: 0.053\n",
      "Epoch 303: Train_Loss: 5.422, Test_Loss: 0.053\n",
      "Epoch 304: Train_Loss: 5.453, Test_Loss: 0.053\n",
      "Epoch 305: Train_Loss: 5.482, Test_Loss: 0.053\n",
      "Epoch 306: Train_Loss: 5.525, Test_Loss: 0.054\n",
      "Epoch 307: Train_Loss: 5.560, Test_Loss: 0.054\n",
      "Epoch 308: Train_Loss: 5.583, Test_Loss: 0.054\n",
      "Epoch 309: Train_Loss: 5.607, Test_Loss: 0.054\n",
      "Epoch 310: Train_Loss: 5.606, Test_Loss: 0.055\n",
      "Epoch 311: Train_Loss: 5.620, Test_Loss: 0.054\n",
      "Epoch 312: Train_Loss: 5.649, Test_Loss: 0.054\n",
      "Epoch 313: Train_Loss: 5.644, Test_Loss: 0.055\n",
      "Epoch 314: Train_Loss: 5.602, Test_Loss: 0.053\n",
      "Epoch 315: Train_Loss: 5.559, Test_Loss: 0.053\n",
      "Epoch 316: Train_Loss: 5.532, Test_Loss: 0.053\n",
      "Epoch 317: Train_Loss: 5.508, Test_Loss: 0.053\n",
      "Epoch 318: Train_Loss: 5.491, Test_Loss: 0.053\n",
      "Epoch 319: Train_Loss: 5.484, Test_Loss: 0.053\n",
      "Epoch 320: Train_Loss: 5.485, Test_Loss: 0.053\n",
      "Epoch 321: Train_Loss: 5.494, Test_Loss: 0.053\n",
      "Epoch 322: Train_Loss: 5.509, Test_Loss: 0.053\n",
      "Epoch 323: Train_Loss: 5.525, Test_Loss: 0.053\n",
      "Epoch 324: Train_Loss: 5.537, Test_Loss: 0.053\n",
      "Epoch 325: Train_Loss: 5.542, Test_Loss: 0.054\n",
      "Epoch 326: Train_Loss: 5.534, Test_Loss: 0.054\n",
      "Epoch 327: Train_Loss: 5.515, Test_Loss: 0.054\n",
      "Epoch 328: Train_Loss: 5.490, Test_Loss: 0.054\n",
      "Epoch 329: Train_Loss: 5.470, Test_Loss: 0.053\n",
      "Epoch 330: Train_Loss: 5.457, Test_Loss: 0.053\n",
      "Epoch 331: Train_Loss: 5.448, Test_Loss: 0.053\n",
      "Epoch 332: Train_Loss: 5.440, Test_Loss: 0.053\n",
      "Epoch 333: Train_Loss: 5.433, Test_Loss: 0.053\n",
      "Epoch 334: Train_Loss: 5.427, Test_Loss: 0.053\n",
      "Epoch 335: Train_Loss: 5.420, Test_Loss: 0.053\n",
      "Epoch 336: Train_Loss: 5.415, Test_Loss: 0.053\n",
      "Epoch 337: Train_Loss: 5.410, Test_Loss: 0.053\n",
      "Epoch 338: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 339: Train_Loss: 5.408, Test_Loss: 0.053\n",
      "Epoch 340: Train_Loss: 5.412, Test_Loss: 0.053\n",
      "Epoch 341: Train_Loss: 5.418, Test_Loss: 0.053\n",
      "Epoch 342: Train_Loss: 5.426, Test_Loss: 0.053\n",
      "Epoch 343: Train_Loss: 5.434, Test_Loss: 0.053\n",
      "Epoch 344: Train_Loss: 5.441, Test_Loss: 0.053\n",
      "Epoch 345: Train_Loss: 5.448, Test_Loss: 0.053\n",
      "Epoch 346: Train_Loss: 5.461, Test_Loss: 0.053\n",
      "Epoch 347: Train_Loss: 5.484, Test_Loss: 0.054\n",
      "Epoch 348: Train_Loss: 5.503, Test_Loss: 0.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349: Train_Loss: 5.499, Test_Loss: 0.055\n",
      "Epoch 350: Train_Loss: 5.487, Test_Loss: 0.054\n",
      "Epoch 351: Train_Loss: 5.487, Test_Loss: 0.054\n",
      "Epoch 352: Train_Loss: 5.497, Test_Loss: 0.054\n",
      "Epoch 353: Train_Loss: 5.510, Test_Loss: 0.054\n",
      "Epoch 354: Train_Loss: 5.529, Test_Loss: 0.054\n",
      "Epoch 355: Train_Loss: 5.566, Test_Loss: 0.055\n",
      "Epoch 356: Train_Loss: 5.645, Test_Loss: 0.057\n",
      "Epoch 357: Train_Loss: 5.807, Test_Loss: 0.061\n",
      "Epoch 358: Train_Loss: 6.134, Test_Loss: 0.061\n",
      "Epoch 359: Train_Loss: 6.290, Test_Loss: 0.054\n",
      "Epoch 360: Train_Loss: 5.914, Test_Loss: 0.053\n",
      "Epoch 361: Train_Loss: 5.710, Test_Loss: 0.053\n",
      "Epoch 362: Train_Loss: 5.607, Test_Loss: 0.053\n",
      "Epoch 363: Train_Loss: 5.545, Test_Loss: 0.053\n",
      "Epoch 364: Train_Loss: 5.505, Test_Loss: 0.053\n",
      "Epoch 365: Train_Loss: 5.476, Test_Loss: 0.053\n",
      "Epoch 366: Train_Loss: 5.454, Test_Loss: 0.052\n",
      "Epoch 367: Train_Loss: 5.438, Test_Loss: 0.052\n",
      "Epoch 368: Train_Loss: 5.424, Test_Loss: 0.052\n",
      "Epoch 369: Train_Loss: 5.413, Test_Loss: 0.052\n",
      "Epoch 370: Train_Loss: 5.403, Test_Loss: 0.052\n",
      "Epoch 371: Train_Loss: 5.394, Test_Loss: 0.052\n",
      "Epoch 372: Train_Loss: 5.385, Test_Loss: 0.052\n",
      "Epoch 373: Train_Loss: 5.378, Test_Loss: 0.052\n",
      "Epoch 374: Train_Loss: 5.371, Test_Loss: 0.052\n",
      "Epoch 375: Train_Loss: 5.364, Test_Loss: 0.052\n",
      "Epoch 376: Train_Loss: 5.358, Test_Loss: 0.052\n",
      "Epoch 377: Train_Loss: 5.353, Test_Loss: 0.052\n",
      "Epoch 378: Train_Loss: 5.348, Test_Loss: 0.052\n",
      "Epoch 379: Train_Loss: 5.343, Test_Loss: 0.052\n",
      "Epoch 380: Train_Loss: 5.338, Test_Loss: 0.052\n",
      "Epoch 381: Train_Loss: 5.334, Test_Loss: 0.052\n",
      "Epoch 382: Train_Loss: 5.330, Test_Loss: 0.052\n",
      "Epoch 383: Train_Loss: 5.326, Test_Loss: 0.052\n",
      "Epoch 384: Train_Loss: 5.323, Test_Loss: 0.052\n",
      "Epoch 385: Train_Loss: 5.320, Test_Loss: 0.052\n",
      "Epoch 386: Train_Loss: 5.316, Test_Loss: 0.052\n",
      "Epoch 387: Train_Loss: 5.313, Test_Loss: 0.052\n",
      "Epoch 388: Train_Loss: 5.310, Test_Loss: 0.052\n",
      "Epoch 389: Train_Loss: 5.307, Test_Loss: 0.052\n",
      "Epoch 390: Train_Loss: 5.304, Test_Loss: 0.052\n",
      "Epoch 391: Train_Loss: 5.302, Test_Loss: 0.052\n",
      "Epoch 392: Train_Loss: 5.299, Test_Loss: 0.052\n",
      "Epoch 393: Train_Loss: 5.296, Test_Loss: 0.052\n",
      "Epoch 394: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 395: Train_Loss: 5.291, Test_Loss: 0.052\n",
      "Epoch 396: Train_Loss: 5.289, Test_Loss: 0.052\n",
      "Epoch 397: Train_Loss: 5.287, Test_Loss: 0.052\n",
      "Epoch 398: Train_Loss: 5.284, Test_Loss: 0.052\n",
      "Epoch 399: Train_Loss: 5.282, Test_Loss: 0.052\n",
      "Epoch 400: Train_Loss: 5.281, Test_Loss: 0.052\n",
      "Epoch 401: Train_Loss: 5.279, Test_Loss: 0.052\n",
      "Epoch 402: Train_Loss: 5.277, Test_Loss: 0.052\n",
      "Epoch 403: Train_Loss: 5.276, Test_Loss: 0.052\n",
      "Epoch 404: Train_Loss: 5.275, Test_Loss: 0.052\n",
      "Epoch 405: Train_Loss: 5.274, Test_Loss: 0.052\n",
      "Epoch 406: Train_Loss: 5.275, Test_Loss: 0.052\n",
      "Epoch 407: Train_Loss: 5.281, Test_Loss: 0.052\n",
      "Epoch 408: Train_Loss: 5.292, Test_Loss: 0.052\n",
      "Epoch 409: Train_Loss: 5.305, Test_Loss: 0.052\n",
      "Epoch 410: Train_Loss: 5.296, Test_Loss: 0.052\n",
      "Epoch 411: Train_Loss: 5.291, Test_Loss: 0.052\n",
      "Epoch 412: Train_Loss: 5.301, Test_Loss: 0.052\n",
      "Epoch 413: Train_Loss: 5.314, Test_Loss: 0.052\n",
      "Epoch 414: Train_Loss: 5.316, Test_Loss: 0.052\n",
      "Epoch 415: Train_Loss: 5.310, Test_Loss: 0.052\n",
      "Epoch 416: Train_Loss: 5.304, Test_Loss: 0.052\n",
      "Epoch 417: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 418: Train_Loss: 5.295, Test_Loss: 0.052\n",
      "Epoch 419: Train_Loss: 5.300, Test_Loss: 0.052\n",
      "Epoch 420: Train_Loss: 5.315, Test_Loss: 0.052\n",
      "Epoch 421: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 422: Train_Loss: 5.356, Test_Loss: 0.053\n",
      "Epoch 423: Train_Loss: 5.388, Test_Loss: 0.053\n",
      "Epoch 424: Train_Loss: 5.424, Test_Loss: 0.054\n",
      "Epoch 425: Train_Loss: 5.442, Test_Loss: 0.056\n",
      "Epoch 426: Train_Loss: 5.476, Test_Loss: 0.054\n",
      "Epoch 427: Train_Loss: 5.480, Test_Loss: 0.053\n",
      "Epoch 428: Train_Loss: 5.406, Test_Loss: 0.053\n",
      "Epoch 429: Train_Loss: 5.353, Test_Loss: 0.052\n",
      "Epoch 430: Train_Loss: 5.344, Test_Loss: 0.052\n",
      "Epoch 431: Train_Loss: 5.366, Test_Loss: 0.053\n",
      "Epoch 432: Train_Loss: 5.378, Test_Loss: 0.053\n",
      "Epoch 433: Train_Loss: 5.368, Test_Loss: 0.053\n",
      "Epoch 434: Train_Loss: 5.361, Test_Loss: 0.052\n",
      "Epoch 435: Train_Loss: 5.363, Test_Loss: 0.053\n",
      "Epoch 436: Train_Loss: 5.368, Test_Loss: 0.053\n",
      "Epoch 437: Train_Loss: 5.360, Test_Loss: 0.053\n",
      "Epoch 438: Train_Loss: 5.346, Test_Loss: 0.053\n",
      "Epoch 439: Train_Loss: 5.342, Test_Loss: 0.053\n",
      "Epoch 440: Train_Loss: 5.340, Test_Loss: 0.053\n",
      "Epoch 441: Train_Loss: 5.333, Test_Loss: 0.053\n",
      "Epoch 442: Train_Loss: 5.329, Test_Loss: 0.053\n",
      "Epoch 443: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 444: Train_Loss: 5.352, Test_Loss: 0.052\n",
      "Epoch 445: Train_Loss: 5.363, Test_Loss: 0.052\n",
      "Epoch 446: Train_Loss: 5.365, Test_Loss: 0.052\n",
      "Epoch 447: Train_Loss: 5.352, Test_Loss: 0.052\n",
      "Epoch 448: Train_Loss: 5.334, Test_Loss: 0.052\n",
      "Epoch 449: Train_Loss: 5.323, Test_Loss: 0.052\n",
      "Epoch 450: Train_Loss: 5.321, Test_Loss: 0.052\n",
      "Epoch 451: Train_Loss: 5.326, Test_Loss: 0.053\n",
      "Epoch 452: Train_Loss: 5.328, Test_Loss: 0.053\n",
      "Epoch 453: Train_Loss: 5.324, Test_Loss: 0.053\n",
      "Epoch 454: Train_Loss: 5.316, Test_Loss: 0.053\n",
      "Epoch 455: Train_Loss: 5.313, Test_Loss: 0.052\n",
      "Epoch 456: Train_Loss: 5.319, Test_Loss: 0.052\n",
      "Epoch 457: Train_Loss: 5.328, Test_Loss: 0.052\n",
      "Epoch 458: Train_Loss: 5.333, Test_Loss: 0.052\n",
      "Epoch 459: Train_Loss: 5.332, Test_Loss: 0.052\n",
      "Epoch 460: Train_Loss: 5.332, Test_Loss: 0.052\n",
      "Epoch 461: Train_Loss: 5.338, Test_Loss: 0.053\n",
      "Epoch 462: Train_Loss: 5.344, Test_Loss: 0.053\n",
      "Epoch 463: Train_Loss: 5.349, Test_Loss: 0.054\n",
      "Epoch 464: Train_Loss: 5.359, Test_Loss: 0.054\n",
      "Epoch 465: Train_Loss: 5.372, Test_Loss: 0.055\n",
      "Epoch 466: Train_Loss: 5.388, Test_Loss: 0.056\n",
      "Epoch 467: Train_Loss: 5.418, Test_Loss: 0.057\n",
      "Epoch 468: Train_Loss: 5.473, Test_Loss: 0.059\n",
      "Epoch 469: Train_Loss: 5.583, Test_Loss: 0.062\n",
      "Epoch 470: Train_Loss: 5.772, Test_Loss: 0.064\n",
      "Epoch 471: Train_Loss: 6.014, Test_Loss: 0.056\n",
      "Epoch 472: Train_Loss: 5.982, Test_Loss: 0.054\n",
      "Epoch 473: Train_Loss: 5.759, Test_Loss: 0.053\n",
      "Epoch 474: Train_Loss: 5.575, Test_Loss: 0.053\n",
      "Epoch 475: Train_Loss: 5.464, Test_Loss: 0.052\n",
      "Epoch 476: Train_Loss: 5.402, Test_Loss: 0.052\n",
      "Epoch 477: Train_Loss: 5.363, Test_Loss: 0.052\n",
      "Epoch 478: Train_Loss: 5.335, Test_Loss: 0.052\n",
      "Epoch 479: Train_Loss: 5.312, Test_Loss: 0.052\n",
      "Epoch 480: Train_Loss: 5.294, Test_Loss: 0.052\n",
      "Epoch 481: Train_Loss: 5.280, Test_Loss: 0.052\n",
      "Epoch 482: Train_Loss: 5.268, Test_Loss: 0.052\n",
      "Epoch 483: Train_Loss: 5.259, Test_Loss: 0.052\n",
      "Epoch 484: Train_Loss: 5.251, Test_Loss: 0.052\n",
      "Epoch 485: Train_Loss: 5.244, Test_Loss: 0.052\n",
      "Epoch 486: Train_Loss: 5.238, Test_Loss: 0.052\n",
      "Epoch 487: Train_Loss: 5.233, Test_Loss: 0.052\n",
      "Epoch 488: Train_Loss: 5.229, Test_Loss: 0.052\n",
      "Epoch 489: Train_Loss: 5.225, Test_Loss: 0.052\n",
      "Epoch 490: Train_Loss: 5.221, Test_Loss: 0.052\n",
      "Epoch 491: Train_Loss: 5.217, Test_Loss: 0.052\n",
      "Epoch 492: Train_Loss: 5.214, Test_Loss: 0.052\n",
      "Epoch 493: Train_Loss: 5.211, Test_Loss: 0.052\n",
      "Epoch 494: Train_Loss: 5.208, Test_Loss: 0.052\n",
      "Epoch 495: Train_Loss: 5.205, Test_Loss: 0.052\n",
      "Epoch 496: Train_Loss: 5.203, Test_Loss: 0.052\n",
      "Epoch 497: Train_Loss: 5.201, Test_Loss: 0.052\n",
      "Epoch 498: Train_Loss: 5.198, Test_Loss: 0.052\n",
      "Epoch 499: Train_Loss: 5.197, Test_Loss: 0.052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 500\n",
    "net.load_weights('easy_checkpoint')\n",
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step(net, example, opt)\n",
    "        total_loss += loss\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    \n",
    "    test_loss = test(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss/len(data), test_loss))\n",
    "    \n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('easy_checkpoint')\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_weights('easy_checkpoint')\n",
    "em = tf.constant(test_data[10].reshape(1,64,64,1))\n",
    "ps = net(tf.cast(em,dtype=tf.float32))\n",
    "\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[10].reshape(64,64)\n",
    "ps[ps>1]=1\n",
    "ps[ps<=0.01]=0\n",
    "# ps[ps>=0.6] = 1\n",
    "# ps[ps<0.6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f67b8422b90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f67b82ff790>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
