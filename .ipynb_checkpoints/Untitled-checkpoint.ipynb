{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from bspnet2D import BspNet2D\n",
    "from loss_func import stage1_loss, rec_loss\n",
    "batchsize = 32\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['pixels']>\n",
      "Keys: <KeysViewHDF5 ['pixels']>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('complex_elements.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])\n",
    "    \n",
    "    \n",
    "with h5py.File('complex_elements_test.hdf5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    test_data = list(f[a_group_key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 2000\n",
      "Testing Dataset Length: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Length: %d\" % len(data))\n",
    "print(\"Testing Dataset Length: %d\" % len(test_data))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batchsize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BspNet2D(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = net(example)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        loss = stage1_loss(output, F, net.decoder.L2.weight, net.decoder.L3.weight) \n",
    "    return loss, tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def test(net, test_ds):\n",
    "    total_loss = 0\n",
    "    for data in test_ds:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        F = tf.reshape(example, [-1, 64*64, 1])\n",
    "        S = net(example)\n",
    "        loss = rec_loss(S, F)\n",
    "        total_loss += loss\n",
    "    return total_loss/len(test_data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.18293534, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 000: Train_Loss: 3.946, Test_Loss: 0.201\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 001: Train_Loss: 3.942, Test_Loss: 0.226\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 002: Train_Loss: 3.938, Test_Loss: 0.258\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev_test_loss = test(net, test_dataset)\n",
    "print(prev_test_loss)\n",
    "for i in range(epoch):\n",
    "    total_loss = 0\n",
    "    for data in train_dataset:\n",
    "        example = tf.cast(data, tf.float32)\n",
    "        loss, grads = train_step(net, example, opt)\n",
    "        total_loss += loss\n",
    "        opt.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    \n",
    "    test_loss = test(net, test_dataset)\n",
    "    print(\"Epoch {:03d}: Train_Loss: {:.3f}, Test_Loss: {:.3f}\".format(i, total_loss/len(data), test_loss))\n",
    "    \n",
    "    if test_loss < prev_test_loss:\n",
    "        net.save_weights('easy_checkpoint')\n",
    "        prev_test_loss = test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_weights('easy_checkpoint')\n",
    "em = tf.constant(test_data[10].reshape(1,64,64,1))\n",
    "ps = net(tf.cast(em,dtype=tf.float32))\n",
    "\n",
    "ps = (ps.numpy()).reshape(64,64)\n",
    "em = test_data[10].reshape(64,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
